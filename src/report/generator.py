"""Markdown æŠ¥å‘Šç”Ÿæˆå™¨"""

from datetime import datetime
from typing import Dict, List, Any
from pathlib import Path
import numpy as np

# å¯¼å…¥å…±äº«çš„æ ¼å¼åŒ–å™¨
from .formatters import (
    ScoreFormatter,
    GradeFormatter,
    TableFormatter,
    ProgressFormatter,
    DimensionTranslator
)


class ReportGenerator:
    """Markdown æŠ¥å‘Šç”Ÿæˆå™¨"""

    def __init__(self, config: Dict[str, Any]):
        """
        åˆå§‹åŒ–æŠ¥å‘Šç”Ÿæˆå™¨

        Args:
            config: æŠ¥å‘Šé…ç½®
        """
        self.config = config
        self.timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")

        # ç¡®ä¿è¾“å‡ºç›®å½•å­˜åœ¨
        self.output_dir = Path("results/reports")
        self.output_dir.mkdir(parents=True, exist_ok=True)

    # ========== Markdown æ ¼å¼è¾…åŠ©æ–¹æ³• ==========
    # æ³¨æ„ï¼šä»¥ä¸‹æ–¹æ³•å·²è¿ç§»åˆ° formatters.pyï¼Œè¿™é‡Œä¿ç•™ä¸ºå…¼å®¹æ€§åŒ…è£…å™¨

    def _format_winning_score(self, value: float, values: List[float],
                              is_lower_better: bool = False, add_trophy: bool = True) -> str:
        """ä¸ºè·èƒœåˆ†æ•°æ·»åŠ æ ¼å¼åŒ–ï¼ˆä½¿ç”¨ ScoreFormatterï¼‰"""
        return ScoreFormatter.format_winning_score(value, values, is_lower_better, add_trophy)

    def _format_winning_model(self, model: str, is_winner: bool) -> str:
        """ä½¿ç”¨æ ¼å¼åŒ–çªå‡ºè·èƒœæ¨¡å‹ï¼ˆä½¿ç”¨ ScoreFormatterï¼‰"""
        return ScoreFormatter.format_winning_model(model, is_winner)

    def _get_grade_emoji(self, score: float) -> str:
        """
        æ ¹æ®åˆ†æ•°è¿”å›ç­‰çº§ emoji å’Œæ–‡å­—è¯´æ˜ï¼ˆä½¿ç”¨ GradeFormatterï¼‰

        Args:
            score: åˆ†æ•° (0-10 æˆ– 0-5)

        Returns:
            ç­‰çº§å­—ç¬¦ä¸²ï¼Œå¦‚ "ğŸŸ¢ ä¼˜ç§€" æˆ– "ğŸ”´ ä¸åˆæ ¼"
        """
        # åˆ¤æ–­æ˜¯ 10 åˆ†åˆ¶è¿˜æ˜¯ 5 åˆ†åˆ¶
        is_10_scale = score > 5.0
        return GradeFormatter.format_grade_with_emoji(score, is_10_scale)

    def generate_report(
        self,
        statistics: Dict[str, Any],
        summaries: List[Any],
        raw_results: List[Dict[str, Any]],
        model_names: List[str],
        test_categories: List[str],
        quality_stats: Dict[str, Any] = None,
        minimax_stats: Dict[str, Any] = None
    ):
        """
        ç”Ÿæˆ Markdown æŠ¥å‘Š

        Args:
            statistics: ç»Ÿè®¡ä¿¡æ¯
            summaries: ç±»åˆ«æ±‡æ€»æ•°æ®
            raw_results: åŸå§‹ç»“æœæ•°æ®
            model_names: æ¨¡å‹åç§°åˆ—è¡¨
            test_categories: æµ‹è¯•ç±»åˆ«åˆ—è¡¨
            quality_stats: è´¨é‡è¯„ä¼°ç»Ÿè®¡æ•°æ®
            minimax_stats: MiniMax è¯„æµ‹ç»Ÿè®¡æ•°æ®
        """
        # è½¬æ¢æ±‡æ€»æ•°æ®ä¸ºå­—å…¸
        summaries_dict = [s.to_dict() for s in summaries]

        # ç”Ÿæˆå®Œæ•´æŠ¥å‘Š
        md_content = self._build_complete_report(
            statistics=statistics,
            summaries=summaries_dict,
            raw_results=raw_results,
            model_names=model_names,
            test_categories=test_categories,
            quality_stats=quality_stats,
            minimax_stats=minimax_stats
        )

        # ä¿å­˜æ–‡ä»¶
        output_path = self.output_dir / f"benchmark_report_{self.timestamp}.md"
        with open(output_path, 'w', encoding='utf-8') as f:
            f.write(md_content)

        print(f"\n[OK] Markdown report generated: {output_path}")

        # åŒæ—¶ä¿å­˜åŸå§‹æ•°æ®ï¼ˆä¾›å‚è€ƒï¼‰
        self._save_raw_data(statistics, summaries_dict, raw_results, quality_stats, minimax_stats)

        return output_path

    def _save_raw_data(
        self,
        statistics: Dict[str, Any],
        summaries: List[Dict[str, Any]],
        raw_results: List[Dict[str, Any]],
        quality_stats: Dict[str, Any] = None,
        minimax_stats: Dict[str, Any] = None
    ):
        """ä¿å­˜åŸå§‹æ•°æ®ï¼ˆç”¨äºåç»­åˆ†æï¼‰"""
        import json

        data_dir = Path("results/data")
        data_dir.mkdir(parents=True, exist_ok=True)

        data = {
            "timestamp": self.timestamp,
            "statistics": statistics,
            "summaries": summaries,
            "raw_results": raw_results,
            "quality_stats": quality_stats,
            "minimax_stats": minimax_stats
        }

        output_path = data_dir / f"raw_data_{self.timestamp}.json"
        with open(output_path, 'w', encoding='utf-8') as f:
            json.dump(data, f, ensure_ascii=False, indent=2)

    def _build_complete_report(
        self,
        statistics: Dict[str, Any],
        summaries: List[Dict[str, Any]],
        raw_results: List[Dict[str, Any]],
        model_names: List[str],
        test_categories: List[str],
        quality_stats: Dict[str, Any] = None,
        minimax_stats: Dict[str, Any] = None
    ) -> str:
        """æ„å»ºå®Œæ•´çš„ Markdown æŠ¥å‘Š"""

        md = ""

        # ========== æŠ¥å‘Šæ ‡é¢˜ ==========
        md += self._generate_header()

        # ========== æ‰§è¡Œæ‘˜è¦ ==========
        md += self._generate_executive_summary(
            summaries, model_names, quality_stats, minimax_stats
        )

        # ========== å¿«é€Ÿå¯¹æ¯”è¡¨ ==========
        md += self._generate_quick_comparison(
            summaries, model_names, test_categories, quality_stats, minimax_stats
        )

        # ========== ä¸€ã€æµ‹è¯•è®¾è®¡ ==========
        md += self._generate_test_design(model_names, test_categories)

        # ========== äºŒã€æµ‹è¯•è¿‡ç¨‹ ==========
        md += self._generate_test_process(statistics, test_categories)

        # ========== ä¸‰ã€æµ‹è¯•ç»“æœ ==========
        md += self._generate_test_results(
            statistics, summaries, raw_results, model_names, test_categories
        )

        # ========== å››ã€MiniMax ç¬¬ä¸‰æ–¹è¯„æµ‹ï¼ˆå¦‚æœæœ‰ï¼‰==========
        if minimax_stats:
            md += self._generate_minimax_judge_section(minimax_stats, model_names)

        # ========== äº”ã€è´¨é‡è¯„ä¼°ï¼ˆå¦‚æœæœ‰ï¼‰==========
        if quality_stats:
            md += self._generate_quality_assessment(
                quality_stats, model_names, test_categories, raw_results,
                has_minimax=(minimax_stats is not None)
            )

        # ========== å…­ã€æ„è§å»ºè®® ==========
        md += self._generate_recommendations(
            statistics, summaries, model_names, quality_stats, minimax_stats
        )

        # ========== æŠ¥å‘Šå°¾éƒ¨ ==========
        md += self._generate_footer()

        return md

    def _generate_header(self) -> str:
        """ç”ŸæˆæŠ¥å‘Šæ ‡é¢˜"""
        return f"""# DeepSeek-v3.2 vs GLM-4.7 API æ€§èƒ½å¯¹æ¯”æµ‹è¯•æŠ¥å‘Š

**æŠ¥å‘Šç”Ÿæˆæ—¶é—´**: {datetime.now().strftime('%Yå¹´%mæœˆ%dæ—¥ %H:%M:%S')}

**æµ‹è¯•ç›®çš„**: å¯¹æ¯” DeepSeek-v3.2 å’Œ GLM-4.7 ä¸¤ä¸ªæ¨¡å‹åœ¨ Anthropic åè®®ä¸‹çš„ API å“åº”æ€§èƒ½

---

"""

    def _generate_executive_summary(
        self,
        summaries: List[Dict[str, Any]],
        model_names: List[str],
        quality_stats: Dict[str, Any] = None,
        minimax_stats: Dict[str, Any] = None
    ) -> str:
        """ç”Ÿæˆæ‰§è¡Œæ‘˜è¦ - å…³é”®æŒ‡æ ‡ä¸€è§ˆè¡¨"""
        md = "## æ‰§è¡Œæ‘˜è¦\n\n"
        md += "### å…³é”®æŒ‡æ ‡ä¸€è§ˆ\n\n"

        # è®¡ç®—æ€»ä½“æ€§èƒ½ç»Ÿè®¡
        overall_data = {}
        for model in model_names:
            model_summaries = [s for s in summaries if s['model_name'] == model and s['test_count'] > 0]
            if model_summaries:
                overall_data[model] = {
                    'ttft': sum(s['ttft_mean'] for s in model_summaries) / len(model_summaries),
                    'speed': sum(s['speed_mean'] for s in model_summaries) / len(model_summaries)
                }

        # è·å–è´¨é‡è¯„åˆ†ï¼ˆå¦‚æœæœ‰ï¼‰
        quality_scores = {}
        if quality_stats and 'by_model' in quality_stats:
            for model in model_names:
                if model in quality_stats['by_model']:
                    model_quality = quality_stats['by_model'][model]
                    # è®¡ç®—å¹³å‡è´¨é‡åˆ†æ•°ï¼ˆæ‰€æœ‰ judge çš„å¹³å‡å€¼ï¼‰
                    scores = []
                    for judge_data in model_quality.values():
                        if isinstance(judge_data, dict) and 'avg_score' in judge_data:
                            scores.append(judge_data['avg_score'])
                    if scores:
                        quality_scores[model] = sum(scores) / len(scores)

        # è·å– MiniMax è¯„åˆ†ï¼ˆå¦‚æœæœ‰ï¼‰
        minimax_scores = {}
        if minimax_stats and 'by_model' in minimax_stats:
            for model in model_names:
                if model in minimax_stats['by_model'] and 'minimax_judge' in minimax_stats['by_model'][model]:
                    minimax_scores[model] = minimax_stats['by_model'][model]['minimax_judge']['overall_score']

        # æ„å»ºå…³é”®æŒ‡æ ‡è¡¨
        md += "| æŒ‡æ ‡ç±»åˆ« | " + " | ".join(model_names) + " |\n"
        md += "|----------|" + "|".join(["----------"] * len(model_names)) + "|\n"

        # TTFT è¡Œ
        if overall_data:
            ttfts = [overall_data[m]['ttft'] for m in model_names if m in overall_data]
            ttft_row = ["**TTFT**ï¼ˆé¦–æ¬¡å“åº”ï¼‰"]
            for model in model_names:
                if model in overall_data:
                    ttft_row.append(self._format_winning_score(overall_data[model]['ttft'], ttfts, is_lower_better=True) + " ms")
                else:
                    ttft_row.append("N/A")
            md += "| " + " | ".join(ttft_row) + " |\n"

        # ç”Ÿæˆé€Ÿåº¦è¡Œ
        if overall_data:
            speeds = [overall_data[m]['speed'] for m in model_names if m in overall_data]
            speed_row = ["**ç”Ÿæˆé€Ÿåº¦**"]
            for model in model_names:
                if model in overall_data:
                    speed_row.append(self._format_winning_score(overall_data[model]['speed'], speeds, is_lower_better=False) + " t/s")
                else:
                    speed_row.append("N/A")
            md += "| " + " | ".join(speed_row) + " |\n"

        # MiniMax æ€»åˆ†è¡Œ
        if minimax_scores:
            mm_scores = [minimax_scores[m] for m in model_names if m in minimax_scores]
            mm_row = ["**MiniMax æ€»åˆ†**"]
            for model in model_names:
                if model in minimax_scores:
                    mm_row.append(self._format_winning_score(minimax_scores[model], mm_scores, is_lower_better=False))
                else:
                    mm_row.append("N/A")
            md += "| " + " | ".join(mm_row) + " |\n"

        # è´¨é‡è¯„åˆ†è¡Œ
        if quality_scores:
            q_scores = [quality_scores[m] for m in model_names if m in quality_scores]
            q_row = ["**è´¨é‡è¯„åˆ†**"]
            for model in model_names:
                if model in quality_scores:
                    q_row.append(self._format_winning_score(quality_scores[model], q_scores, is_lower_better=False))
                else:
                    q_row.append("N/A")
            md += "| " + " | ".join(q_row) + " |\n"

        md += "\n*æ³¨ï¼šğŸ† è¡¨ç¤ºè¯¥æŒ‡æ ‡æœ€ä½³æ¨¡å‹ï¼Œç²—ä½“è¡¨ç¤ºè·èƒœã€‚TTFT è¶Šå°è¶Šå¥½ï¼Œå…¶ä»–æŒ‡æ ‡è¶Šå¤§è¶Šå¥½*\n\n"

        # æ ¸å¿ƒå‘ç°
        md += "### æ ¸å¿ƒå‘ç°\n\n"

        # è®¡ç®—æ€§èƒ½é¢†å…ˆç™¾åˆ†æ¯”
        if len(overall_data) >= 2:
            model1, model2 = model_names[0], model_names[1]
            if model1 in overall_data and model2 in overall_data:
                # TTFT é¢†å…ˆç™¾åˆ†æ¯”
                ttft_pct = (overall_data[model2]['ttft'] - overall_data[model1]['ttft']) / overall_data[model2]['ttft'] * 100
                ttft_leader = model1 if ttft_pct > 0 else model2
                ttft_pct = abs(ttft_pct)

                # é€Ÿåº¦é¢†å…ˆç™¾åˆ†æ¯”
                speed_pct = (overall_data[model1]['speed'] - overall_data[model2]['speed']) / overall_data[model2]['speed'] * 100
                speed_leader = model1 if speed_pct > 0 else model2
                speed_pct = abs(speed_pct)

                md += f"- ğŸš€ **æ€§èƒ½é¢†å…ˆ**ï¼š{ttft_leader} çš„é¦–æ¬¡å“åº”æ—¶é—´å¿« {ttft_pct:.1f}%ï¼Œç”Ÿæˆé€Ÿåº¦å¿« {speed_pct:.1f}%\n"

        # è´¨é‡æ¯”è¾ƒ
        if quality_scores and len(quality_scores) >= 2:
            model1, model2 = model_names[0], model_names[1]
            if model1 in quality_scores and model2 in quality_scores:
                q_diff = quality_scores[model1] - quality_scores[model2]
                if abs(q_diff) > 0.1:
                    q_leader = model1 if q_diff > 0 else model2
                    q_pct = abs(q_diff) / min(quality_scores.values()) * 100
                    md += f"- ğŸ¯ **è´¨é‡ä¼˜åŠ¿**ï¼š{q_leader} çš„è´¨é‡è¯„åˆ†é«˜ {q_pct:.1f}%\n"

        # MiniMax è´¨é‡æ¯”è¾ƒ
        if minimax_scores and len(minimax_scores) >= 2:
            model1, model2 = model_names[0], model_names[1]
            if model1 in minimax_scores and model2 in minimax_scores:
                mm_diff = minimax_scores[model1] - minimax_scores[model2]
                if abs(mm_diff) > 0.1:
                    mm_leader = model1 if mm_diff > 0 else model2
                    md += f"- ğŸ† **MiniMax è¯„æµ‹**ï¼š{mm_leader} åœ¨ç¬¬ä¸‰æ–¹è¯„æµ‹ä¸­è¡¨ç°æ›´ä¼˜\n"

        md += "\n---\n\n"
        return md

    def _generate_quick_comparison(
        self,
        summaries: List[Dict[str, Any]],
        model_names: List[str],
        test_categories: List[str],
        quality_stats: Dict[str, Any] = None,
        minimax_stats: Dict[str, Any] = None
    ) -> str:
        """ç”Ÿæˆå¿«é€Ÿå¯¹æ¯”è¡¨ - æŒ‰åœºæ™¯æ¨è"""
        md = "### å¿«é€Ÿå¯¹æ¯”è¡¨\n\n"
        md += "| åœºæ™¯ | æ¨èæ¨¡å‹ | ç†ç”± | ç½®ä¿¡åº¦ |\n"
        md += "|------|----------|------|--------|\n"

        # æ”¶é›†å„åœºæ™¯çš„æ•°æ®
        category_data = {}
        for category in test_categories:
            for model in model_names:
                model_data = [s for s in summaries if s['category'] == category and s['model_name'] == model]
                if model_data and model_data[0]['test_count'] > 0:
                    if category not in category_data:
                        category_data[category] = {}
                    category_data[category][model] = model_data[0]

        # ä¸ºæ¯ä¸ªåœºæ™¯ç”Ÿæˆæ¨è
        for category in test_categories:
            if category not in category_data or len(category_data[category]) < 2:
                continue

            models_in_category = list(category_data[category].keys())
            model1, model2 = models_in_category[0], models_in_category[1]
            data1, data2 = category_data[category][model1], category_data[category][model2]

            # è®¡ç®—ç»¼åˆå¾—åˆ†ï¼ˆTTFT å’Œé€Ÿåº¦çš„åŠ æƒï¼‰
            # å½’ä¸€åŒ– TTFTï¼ˆè¶Šå°è¶Šå¥½ï¼‰
            ttft_sum = data1['ttft_mean'] + data2['ttft_mean']
            ttft_score1 = (ttft_sum - data1['ttft_mean']) / ttft_sum  # TTFT è¶Šå°åˆ†æ•°è¶Šé«˜
            ttft_score2 = (ttft_sum - data2['ttft_mean']) / ttft_sum

            # å½’ä¸€åŒ–é€Ÿåº¦ï¼ˆè¶Šå¤§è¶Šå¥½ï¼‰
            speed_sum = data1['speed_mean'] + data2['speed_mean']
            speed_score1 = data1['speed_mean'] / speed_sum
            speed_score2 = data2['speed_mean'] / speed_sum

            # ç»¼åˆå¾—åˆ†ï¼ˆTTFT 40%ï¼Œé€Ÿåº¦ 60%ï¼‰
            score1 = ttft_score1 * 0.4 + speed_score1 * 0.6
            score2 = ttft_score2 * 0.4 + speed_score2 * 0.6

            winner = model1 if score1 > score2 else model2
            margin = abs(score1 - score2) * 100

            # ç¡®å®šç†ç”±
            if data1['ttft_mean'] < data2['ttft_mean'] and data1['speed_mean'] > data2['speed_mean']:
                reason = f"TTFT å¿« {abs(data1['ttft_mean'] - data2['ttft_mean']) / data2['ttft_mean'] * 100:.1f}%ï¼Œé€Ÿåº¦å¿« {abs(data1['speed_mean'] - data2['speed_mean']) / data2['speed_mean'] * 100:.1f}%"
            elif data1['ttft_mean'] < data2['ttft_mean']:
                reason = f"TTFT å¿« {abs(data1['ttft_mean'] - data2['ttft_mean']) / data2['ttft_mean'] * 100:.1f}%"
            elif data1['speed_mean'] > data2['speed_mean']:
                reason = f"é€Ÿåº¦å¿« {abs(data1['speed_mean'] - data2['speed_mean']) / data2['speed_mean'] * 100:.1f}%"
            else:
                reason = "ç»¼åˆæ€§èƒ½æ›´ä¼˜"

            # ç¡®å®šç½®ä¿¡åº¦
            if margin > 15:
                confidence = "*** é«˜åº¦æ˜¾è‘—"
            elif margin > 8:
                confidence = "** æ˜¾è‘—"
            elif margin > 3:
                confidence = "* è¾¹ç¼˜æ˜¾è‘—"
            else:
                confidence = "ç›¸å½“"

            md += f"| {category} | **{winner}** | {reason} | {confidence} |\n"

        md += "\n*æ³¨ï¼š*** é«˜åº¦æ˜¾è‘—ï¼ˆå·®å¼‚ >15%ï¼‰ï¼Œ** æ˜¾è‘—ï¼ˆå·®å¼‚ >8%ï¼‰ï¼Œ* è¾¹ç¼˜æ˜¾è‘—ï¼ˆå·®å¼‚ >3%ï¼‰*\n\n"
        md += "---\n\n"
        return md

    def _generate_minimax_judge_section(self, minimax_stats: Dict[str, Any],
                                       model_names: List[str]) -> str:
        """
        ç”Ÿæˆ MiniMax ç¬¬ä¸‰æ–¹è¯„æµ‹ç« èŠ‚

        Args:
            minimax_stats: æ¥è‡ª JSON çš„ MiniMax è¯„æµ‹ç»Ÿè®¡æ•°æ®
            model_names: è¢«è¯„ä¼°çš„æ¨¡å‹åˆ—è¡¨

        Returns:
            åŒ…å« MiniMax ç»“æœçš„ Markdown ç« èŠ‚
        """
        # æ£€æŸ¥æ˜¯å¦æœ‰ MiniMax æ•°æ®
        if not minimax_stats or "by_judge" not in minimax_stats or "minimax_judge" not in minimax_stats.get("by_judge", {}):
            return ""

        md = "## å››ã€MiniMax ç¬¬ä¸‰æ–¹è¯„æµ‹\n\n"

        # 4.1 MiniMax ç»¼åˆè¯„åˆ†
        md += "### 4.1 MiniMax ç»¼åˆè¯„åˆ† (0-10åˆ†åˆ¶)\n\n"
        md += self._generate_minimax_overall_table(minimax_stats, model_names)

        # 4.2 å››ç»´åº¦è¯¦ç»†è¯„åˆ†
        md += "### 4.2 å››ç»´åº¦è¯¦ç»†è¯„åˆ†\n\n"
        md += self._generate_minimax_dimensions_table(minimax_stats, model_names)

        # 4.3 å­ç»´åº¦èƒ½åŠ›çŸ©é˜µ
        md += "### 4.3 å­ç»´åº¦èƒ½åŠ›çŸ©é˜µ\n\n"
        md += self._generate_minimax_subdimension_heatmap(minimax_stats, model_names)

        md += "\n---\n\n"
        return md

    def _generate_minimax_overall_table(self, minimax_stats: Dict, models: List[str]) -> str:
        """ç”Ÿæˆ MiniMax ç»¼åˆè¯„åˆ†è¡¨"""
        md = "| æ¨¡å‹ | ç»¼åˆå¾—åˆ† | ç­‰çº§ | åŸºç¡€æ€§èƒ½ | æ ¸å¿ƒèƒ½åŠ› | å®ç”¨åœºæ™¯ | é«˜çº§ç‰¹æ€§ |\n"
        md += "|------|---------|------|----------|----------|----------|----------|\n"

        for model in models:
            if model not in minimax_stats.get("by_model", {}):
                continue
            if "minimax_judge" not in minimax_stats["by_model"][model]:
                continue

            model_data = minimax_stats["by_model"][model]["minimax_judge"]
            overall = model_data.get("overall_score", 0)
            dims = model_data.get("dimension_scores", {})

            # è·å–ç­‰çº§ emoji
            grade = self._get_grade_emoji(overall)

            # æ ¼å¼åŒ–è·èƒœåˆ†æ•°ï¼ˆä½¿ç”¨ç²—ä½“ï¼‰
            all_overall_scores = []
            for m in models:
                if m in minimax_stats.get("by_model", {}) and "minimax_judge" in minimax_stats["by_model"][m]:
                    all_overall_scores.append(minimax_stats["by_model"][m]["minimax_judge"].get("overall_score", 0))

            overall_formatted = self._format_winning_score(overall, all_overall_scores, is_lower_better=False)

            # æ ¼å¼åŒ–æ¨¡å‹åç§°
            is_winner = all_overall_scores and overall == max(all_overall_scores)
            model_formatted = self._format_winning_model(model, is_winner)

            md += f"| {model_formatted} | {overall_formatted} | {grade} "

            # å››ç»´åº¦åˆ†æ•°
            for dim in ["basic_performance", "core_capabilities",
                       "practical_scenarios", "advanced_features"]:
                score = dims.get(dim, 0)
                all_scores = []
                for m in models:
                    if m in minimax_stats.get("by_model", {}) and "minimax_judge" in minimax_stats["by_model"][m]:
                        all_scores.append(minimax_stats["by_model"][m]["minimax_judge"].get("dimension_scores", {}).get(dim, 0))
                md += f"| {self._format_winning_score(score, all_scores)} "

            md += "|\n"

        md += "\n*æ³¨ï¼šğŸ† è¡¨ç¤ºè¯¥æŒ‡æ ‡æœ€ä½³æ¨¡å‹ï¼Œç²—ä½“è¡¨ç¤ºè·èƒœ*\n"
        return md

    def _generate_minimax_dimensions_table(self, minimax_stats: Dict, models: List[str]) -> str:
        """ç”Ÿæˆ MiniMax å››ç»´åº¦è¯¦ç»†è¯„åˆ†è¡¨"""
        md = "#### å„ç»´åº¦å¾—åˆ†å¯¹æ¯”\n\n"

        dimensions_cn = {
            "basic_performance": "åŸºç¡€æ€§èƒ½",
            "core_capabilities": "æ ¸å¿ƒèƒ½åŠ›",
            "practical_scenarios": "å®ç”¨åœºæ™¯",
            "advanced_features": "é«˜çº§ç‰¹æ€§"
        }

        for dim_key, dim_name in dimensions_cn.items():
            md += f"**{dim_name}**\n\n"

            # æ”¶é›†è¯¥ç»´åº¦çš„æ•°æ®
            dim_data = {}
            for model in models:
                if model in minimax_stats.get("by_model", {}) and "minimax_judge" in minimax_stats["by_model"][model]:
                    dim_data[model] = minimax_stats["by_model"][model]["minimax_judge"].get("dimension_scores", {}).get(dim_key, 0)

            if not dim_data:
                continue

            # æ‰¾å‡ºæœ€ä½³æ¨¡å‹
            best_model = max(dim_data.items(), key=lambda x: x[1])

            md += f"- æœ€ä½³æ¨¡å‹ï¼š**{best_model[0]}** ({best_model[1]:.2f}/10)\n"

            # è¯¦ç»†å¯¹æ¯”
            for model, score in dim_data.items():
                is_best = score == best_model[1]
                marker = " ğŸ†" if is_best else ""
                formatted_score = f"**{score:.2f}**{marker}" if is_best else f"{score:.2f}"
                md += f"  - {model}: {formatted_score}\n"

            md += "\n"

        return md

    def _generate_minimax_subdimension_heatmap(self, minimax_stats: Dict, models: List[str]) -> str:
        """ç”Ÿæˆ MiniMax å­ç»´åº¦èƒ½åŠ›çƒ­åŠ›å›¾"""
        md = "#### å­ç»´åº¦èƒ½åŠ›çƒ­åŠ›å›¾\n\n"

        # å®šä¹‰å­ç»´åº¦åˆ—è¡¨
        sub_dimensions = [
            ("å®æ—¶å“åº”", "real_time"),
            ("ååé‡", "throughput"),
            ("ç¨³å®šæ€§", "stability"),
            ("é€»è¾‘æ¨ç†", "reasoning"),
            ("ä»£ç ç”Ÿæˆ", "code"),
            ("æ–‡æœ¬ç†è§£", "understanding"),
            ("åˆ›æ„ç”Ÿæˆ", "creativity"),
            ("ä¸“ä¸šåº”ç”¨", "professional"),
            ("ä¸­æ–‡å¤„ç†", "chinese"),
            ("é•¿æ–‡æœ¬", "long_text"),
            ("ç»“æ„åŒ–è¾“å‡º", "structured"),
            ("åˆ›æ–°æ€ç»´", "innovation")
        ]

        md += "| èƒ½åŠ›ç»´åº¦ | " + " | ".join(models) + " |\n"
        md += "|----------|" + "|".join(["---------"] * len(models)) + "|\n"

        for dim_name, _ in sub_dimensions:
            row = [dim_name]

            for model in models:
                if model in minimax_stats.get("by_model", {}) and "minimax_judge" in minimax_stats["by_model"][model]:
                    sub_scores = minimax_stats["by_model"][model]["minimax_judge"].get("sub_dimension_scores", {})
                    score = sub_scores.get(dim_name, 0)

                    # åˆ›å»ºè¿›åº¦æ¡ï¼ˆ10ä¸ªå­—ç¬¦ï¼‰
                    bar_length = int(score / 10 * 10)
                    filled = "â–ˆ" * bar_length
                    empty = "â–‘" * (10 - bar_length)
                    row.append(f"{filled}{empty} {score:.1f}")
                else:
                    row.append("N/A")

            md += "| " + " | ".join(row) + " |\n"

        md += "\n**å›¾ä¾‹**: â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ (9-10åˆ†) â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‘â–‘ (7-8åˆ†) â–ˆâ–ˆâ–ˆâ–ˆâ–‘â–‘â–‘â–‘ (5-6åˆ†) â–ˆâ–ˆâ–‘â–‘â–‘â–‘â–‘â–‘ (3-4åˆ†)\n"
        return md

    def _generate_test_design(self, model_names: List[str], test_categories: List[str]) -> str:
        """ç”Ÿæˆæµ‹è¯•è®¾è®¡éƒ¨åˆ†"""
        md = "## ä¸€ã€æµ‹è¯•è®¾è®¡\n\n"

        # 1.1 æµ‹è¯•å¯¹è±¡
        md += "### 1.1 æµ‹è¯•å¯¹è±¡\n\n"
        md += "æœ¬æ¬¡æµ‹è¯•å¯¹æ¯”ä»¥ä¸‹ä¸¤ä¸ªæ¨¡å‹ï¼š\n\n"
        for i, model in enumerate(model_names, 1):
            md += f"**{i}. {model}**\n\n"
        md += "\n"

        # 1.2 æµ‹è¯•æŒ‡æ ‡
        md += "### 1.2 æµ‹è¯•æŒ‡æ ‡\n\n"
        md += "æœ¬æ¬¡æµ‹è¯•ä¸»è¦å…³æ³¨ä»¥ä¸‹æ€§èƒ½æŒ‡æ ‡ï¼š\n\n"
        md += "| æŒ‡æ ‡ | è¯´æ˜ | å•ä½ |\n"
        md += "|------|------|------|\n"
        md += "| TTFT | Time to First Tokenï¼Œä»å‘é€è¯·æ±‚åˆ°æ”¶åˆ°ç¬¬ä¸€ä¸ª token çš„æ—¶é—´ | æ¯«ç§’ (ms) |\n"
        md += "| æ€»å“åº”æ—¶é—´ | å®Œæ•´è¯·æ±‚ä»å‘é€åˆ°æ¥æ”¶å®Œæˆçš„æ€»æ—¶é—´ | æ¯«ç§’ (ms) |\n"
        md += "| ç”Ÿæˆé€Ÿåº¦ | æ¨¡å‹ç”Ÿæˆ token çš„é€Ÿåº¦ | tokens/ç§’ |\n"
        md += "| Token é—´å»¶è¿Ÿ | è¿ç»­ token ä¹‹é—´çš„å¹³å‡æ—¶é—´ | æ¯«ç§’ (ms) |\n"
        md += "| è¾“å‡º Token æ•° | æ¨¡å‹ç”Ÿæˆçš„ token æ•°é‡ | ä¸ª |\n\n"

        # 1.3 æµ‹è¯•åœºæ™¯
        md += "### 1.3 æµ‹è¯•åœºæ™¯\n\n"
        md += "æµ‹è¯•æ¶µç›–ä»¥ä¸‹åœºæ™¯ç±»åˆ«ï¼š\n\n"

        category_descriptions = {
            "qa_simple": "ç®€å•é—®ç­” - äº‹å®æ€§é—®é¢˜ã€å®šä¹‰æŸ¥è¯¢ã€ç®€å•è§£é‡Š",
            "code_generation": "ä»£ç ç”Ÿæˆ - ç®—æ³•å®ç°ã€API è®¾è®¡ã€è°ƒè¯•åœºæ™¯",
            "reasoning_complex": "å¤æ‚æ¨ç† - å¤šæ­¥é€»è¾‘ã€æ•°å­¦è¯æ˜ã€åˆ†ææ¨ç†",
            "generation_long": "é•¿æ–‡æœ¬ç”Ÿæˆ - è®ºæ–‡å†™ä½œã€æ•…äº‹ç”Ÿæˆã€æ–‡ç« åˆ›ä½œ",
            "summarization": "æ–‡æœ¬æ‘˜è¦ - é•¿æ–‡æ¡£æ‘˜è¦ã€è¦ç‚¹æå–",
            "translation": "ç¿»è¯‘ä»»åŠ¡ - å¤šè¯­è¨€å¯¹ã€ä¸Šä¸‹æ–‡æ„ŸçŸ¥ç¿»è¯‘",
            "math_reasoning": "æ•°å­¦æ¨ç† - åº”ç”¨é¢˜ã€ç¬¦å·æ•°å­¦ã€ç»Ÿè®¡å­¦",
            "creative_writing": "åˆ›æ„å†™ä½œ - è¯—æ­Œã€å°è¯´ã€å¯¹è¯åˆ›ä½œ",
            "factual_accuracy": "äº‹å®å‡†ç¡®æ€§ - äº‹å®éªŒè¯ã€å¹»è§‰æ£€æµ‹",
            "multi_turn": "å¤šè½®å¯¹è¯ - ä¸Šä¸‹æ–‡ä¿æŒã€å¯¹è¯ä¸€è‡´æ€§"
        }

        for category in test_categories:
            desc = category_descriptions.get(category, "å…¶ä»–æµ‹è¯•åœºæ™¯")
            md += f"- **{category}**: {desc}\n"
        md += "\n"

        # 1.4 æµ‹è¯•æ–¹æ³•
        md += "### 1.4 æµ‹è¯•æ–¹æ³•\n\n"
        md += "**æµ‹è¯•æµç¨‹**ï¼š\n\n"
        md += "1. **é¢„çƒ­é˜¶æ®µ**: æ¯ä¸ªæ¨¡å‹å…ˆè¿›è¡Œ 2 æ¬¡é¢„çƒ­è¯·æ±‚ï¼Œç¡®ä¿è¿æ¥å»ºç«‹\n"
        md += "2. **æ­£å¼æµ‹è¯•**: æ¯ä¸ªæµ‹è¯•ç”¨ä¾‹è¿è¡Œ 3 æ¬¡ï¼Œå–å¹³å‡å€¼\n"
        md += "3. **æµå¼å“åº”**: ä½¿ç”¨ Anthropic åè®®çš„æµå¼å“åº”æ¥å£\n"
        md += "4. **é«˜ç²¾åº¦è®¡æ—¶**: ä½¿ç”¨ `time.perf_counter()` è¿›è¡Œçº³ç§’çº§ç²¾åº¦è®¡æ—¶\n\n"

        md += "**æ€§èƒ½æŒ‡æ ‡è®¡ç®—æ–¹å¼**ï¼š\n\n"
        md += "```\n"
        md += "TTFT = é¦–ä¸ª token åˆ°è¾¾æ—¶é—´ - è¯·æ±‚å¼€å§‹æ—¶é—´\n"
        md += "æ€»å“åº”æ—¶é—´ = è¯·æ±‚ç»“æŸæ—¶é—´ - è¯·æ±‚å¼€å§‹æ—¶é—´\n"
        md += "ç”Ÿæˆæ—¶é—´ = æ€»å“åº”æ—¶é—´ - TTFT\n"
        md += "ç”Ÿæˆé€Ÿåº¦ = token æ•°é‡ / ç”Ÿæˆæ—¶é—´\n"
        md += "Token é—´å»¶è¿Ÿ = å¹³å‡ç›¸é‚» token æ—¶é—´é—´éš”\n"
        md += "```\n\n"

        md += "---\n\n"
        return md

    def _generate_test_process(
        self,
        statistics: Dict[str, Any],
        test_categories: List[str]
    ) -> str:
        """ç”Ÿæˆæµ‹è¯•è¿‡ç¨‹éƒ¨åˆ†"""
        md = "## äºŒã€æµ‹è¯•è¿‡ç¨‹\n\n"

        # 2.1 æµ‹è¯•ç¯å¢ƒ
        md += "### 2.1 æµ‹è¯•ç¯å¢ƒ\n\n"
        md += f"- **æµ‹è¯•æ—¶é—´**: {statistics.get('start_time', 'N/A')} ~ {statistics.get('end_time', 'N/A')}\n"
        md += f"- **æµ‹è¯•å·¥å…·**: Python + Anthropic SDK\n"
        md += f"- **ç½‘ç»œç¯å¢ƒ**: æ ‡å‡†äº’è”ç½‘è¿æ¥\n\n"

        # 2.2 æµ‹è¯•æ‰§è¡Œ
        md += "### 2.2 æµ‹è¯•æ‰§è¡Œ\n\n"
        total_tests = statistics.get('total_tests', 0)
        successful_tests = statistics.get('successful_tests', 0)
        failed_tests = statistics.get('failed_tests', 0)
        success_rate = statistics.get('success_rate', 0) * 100

        md += f"- **è®¡åˆ’æµ‹è¯•æ•°**: {total_tests} æ¬¡\n"
        md += f"- **æˆåŠŸæ‰§è¡Œ**: {successful_tests} æ¬¡\n"
        md += f"- **æ‰§è¡Œå¤±è´¥**: {failed_tests} æ¬¡\n"
        md += f"- **æˆåŠŸç‡**: {success_rate:.1f}%\n\n"

        # æŒ‰æ¨¡å‹ç»Ÿè®¡
        md += "**å„æ¨¡å‹æ‰§è¡Œæƒ…å†µ**ï¼š\n\n"
        model_stats = statistics.get('model_stats', {})
        for model_name, stats in model_stats.items():
            md += f"- **{model_name}**:\n"
            md += f"  - æ€»è®¡: {stats.get('total', 0)} æ¬¡\n"
            md += f"  - æˆåŠŸ: {stats.get('success', 0)} æ¬¡\n"
            md += f"  - å¤±è´¥: {stats.get('failed', 0)} æ¬¡\n\n"

        md += "---\n\n"
        return md

    def _generate_test_results(
        self,
        statistics: Dict[str, Any],
        summaries: List[Dict[str, Any]],
        raw_results: List[Dict[str, Any]],
        model_names: List[str],
        test_categories: List[str]
    ) -> str:
        """ç”Ÿæˆæµ‹è¯•ç»“æœéƒ¨åˆ†"""
        md = "## ä¸‰ã€æµ‹è¯•ç»“æœ\n\n"

        # 3.1 æ€»ä½“æ€§èƒ½å¯¹æ¯”
        md += "### 3.1 æ€»ä½“æ€§èƒ½å¯¹æ¯”\n\n"
        md += self._generate_overall_comparison(summaries, model_names)

        # 3.2 å„æŒ‡æ ‡è¯¦ç»†å¯¹æ¯”
        md += "### 3.2 å„æŒ‡æ ‡è¯¦ç»†å¯¹æ¯”\n\n"

        # TTFT å¯¹æ¯”
        md += "#### 3.2.1 TTFT (é¦–æ¬¡å“åº”æ—¶é—´) å¯¹æ¯”\n\n"
        md += self._generate_ttft_comparison(summaries, model_names, test_categories)

        # ç”Ÿæˆé€Ÿåº¦å¯¹æ¯”
        md += "#### 3.2.2 ç”Ÿæˆé€Ÿåº¦å¯¹æ¯”\n\n"
        md += self._generate_speed_comparison(summaries, model_names, test_categories)

        # æ€»å“åº”æ—¶é—´å¯¹æ¯”
        md += "#### 3.2.3 æ€»å“åº”æ—¶é—´å¯¹æ¯”\n\n"
        md += self._generate_total_time_comparison(summaries, model_names, test_categories)

        # 3.3 åˆ†åœºæ™¯æ€§èƒ½åˆ†æ
        md += "### 3.3 åˆ†åœºæ™¯æ€§èƒ½åˆ†æ\n\n"
        md += self._generate_category_analysis(summaries, model_names, test_categories)

        # 3.4 æ¨¡å‹ä¼˜åŠ¿åˆ†æ
        md += "### 3.4 æ¨¡å‹ä¼˜åŠ¿åˆ†æ\n\n"
        md += self._generate_model_advantages(summaries, model_names, test_categories)

        md += "---\n\n"
        return md

    def _generate_overall_comparison(
        self,
        summaries: List[Dict[str, Any]],
        model_names: List[str]
    ) -> str:
        """ç”Ÿæˆæ€»ä½“å¯¹æ¯”è¡¨"""
        md = "**æ€»ä½“å¹³å‡æ€§èƒ½**ï¼š\n\n"

        # è®¡ç®—æ€»ä½“å¹³å‡å€¼
        overall_data = {}
        for model in model_names:
            model_summaries = [s for s in summaries if s['model_name'] == model and s['test_count'] > 0]
            if model_summaries:
                avg_ttft = sum(s['ttft_mean'] for s in model_summaries) / len(model_summaries)
                avg_speed = sum(s['speed_mean'] for s in model_summaries) / len(model_summaries)
                avg_total_time = sum(s['total_time_mean'] for s in model_summaries) / len(model_summaries)

                overall_data[model] = {
                    'ttft': avg_ttft,
                    'speed': avg_speed,
                    'total_time': avg_total_time
                }

        # æ”¶é›†æ‰€æœ‰å€¼ç”¨äºæ¯”è¾ƒ
        all_ttft = [overall_data[m]['ttft'] for m in model_names if m in overall_data]
        all_speeds = [overall_data[m]['speed'] for m in model_names if m in overall_data]
        all_times = [overall_data[m]['total_time'] for m in model_names if m in overall_data]

        # åˆ›å»ºè¡¨æ ¼
        md += "| æ¨¡å‹ | å¹³å‡ TTFT (ms) | å¹³å‡ç”Ÿæˆé€Ÿåº¦ (tokens/s) | å¹³å‡æ€»æ—¶é—´ (ms) |\n"
        md += "|------|----------------|------------------------|---------------|\n"

        for model in model_names:
            if model in overall_data:
                data = overall_data[model]
                # ä½¿ç”¨æ ¼å¼åŒ–æ–¹æ³•çªå‡ºè·èƒœåˆ†æ•°
                ttft_str = self._format_winning_score(data['ttft'], all_ttft, is_lower_better=True)
                speed_str = self._format_winning_score(data['speed'], all_speeds, is_lower_better=False)
                time_str = self._format_winning_score(data['total_time'], all_times, is_lower_better=True)
                md += f"| {model} | {ttft_str} | {speed_str} | {time_str} |\n"

        md += "\n"
        md += "*æ³¨ï¼šç²—ä½“è¡¨ç¤ºè¯¥æŒ‡æ ‡æœ€ä½³æ¨¡å‹ã€‚TTFT å’Œæ€»æ—¶é—´è¶Šå°è¶Šå¥½ï¼Œç”Ÿæˆé€Ÿåº¦è¶Šå¤§è¶Šå¥½*\n\n"
        return md

    def _generate_ttft_comparison(
        self,
        summaries: List[Dict[str, Any]],
        model_names: List[str],
        test_categories: List[str]
    ) -> str:
        """ç”Ÿæˆ TTFT å¯¹æ¯”è¡¨ï¼ˆä½¿ç”¨ Markdown ç²—ä½“çªå‡ºä¼˜åŠ¿å€¼ï¼‰"""
        md = "| æµ‹è¯•ç±»åˆ« | " + " | ".join(model_names) + " |\n"
        md += "|---------|" + "|".join(["---------"] * len(model_names)) + "|\n"

        for category in test_categories:
            row = [category]

            # è·å–å„æ¨¡å‹çš„ TTFT
            model_ttfts = {}
            for model in model_names:
                model_data = [s for s in summaries if s['category'] == category and s['model_name'] == model]
                if model_data and model_data[0]['test_count'] > 0:
                    ttft = model_data[0]['ttft_mean']
                    model_ttfts[model] = ttft

            # æ ¼å¼åŒ– TTFT å€¼ï¼Œçªå‡ºæœ€ä½³å€¼ï¼ˆğŸ† ä¼šè‡ªåŠ¨æ·»åŠ åˆ°è·èƒœå€¼ï¼‰
            for model in model_names:
                if model in model_ttfts:
                    ttft_values = list(model_ttfts.values())
                    formatted_ttft = self._format_winning_score(model_ttfts[model], ttft_values, is_lower_better=True)
                    row.append(formatted_ttft)
                else:
                    row.append("N/A")

            md += "| " + " | ".join(row) + " |\n"

        md += "\n"
        md += "*æ³¨ï¼šTTFT è¶Šå°è¶Šå¥½ï¼Œè¡¨ç¤ºå“åº”è¶Šå¿«ã€‚ğŸ† è¡¨ç¤ºè¯¥æŒ‡æ ‡æœ€ä½³æ¨¡å‹ï¼Œç²—ä½“è¡¨ç¤ºè·èƒœ*\n\n"
        return md

    def _generate_speed_comparison(
        self,
        summaries: List[Dict[str, Any]],
        model_names: List[str],
        test_categories: List[str]
    ) -> str:
        """ç”Ÿæˆç”Ÿæˆé€Ÿåº¦å¯¹æ¯”è¡¨ï¼ˆä½¿ç”¨ Markdown ç²—ä½“çªå‡ºä¼˜åŠ¿å€¼ï¼‰"""
        md = "| æµ‹è¯•ç±»åˆ« | " + " | ".join(model_names) + " |\n"
        md += "|---------|" + "|".join(["---------"] * len(model_names)) + "|\n"

        for category in test_categories:
            row = [category]

            # è·å–å„æ¨¡å‹çš„ç”Ÿæˆé€Ÿåº¦
            model_speeds = {}
            for model in model_names:
                model_data = [s for s in summaries if s['category'] == category and s['model_name'] == model]
                if model_data and model_data[0]['test_count'] > 0:
                    speed = model_data[0]['speed_mean']
                    model_speeds[model] = speed

            # æ ¼å¼åŒ–é€Ÿåº¦å€¼ï¼Œçªå‡ºæœ€ä½³å€¼ï¼ˆğŸ† ä¼šè‡ªåŠ¨æ·»åŠ åˆ°è·èƒœå€¼ï¼‰
            for model in model_names:
                if model in model_speeds:
                    speed_values = list(model_speeds.values())
                    formatted_speed = self._format_winning_score(model_speeds[model], speed_values, is_lower_better=False)
                    row.append(formatted_speed)
                else:
                    row.append("N/A")

            md += "| " + " | ".join(row) + " |\n"

        md += "\n"
        md += "*æ³¨ï¼šç”Ÿæˆé€Ÿåº¦è¶Šå¤§è¶Šå¥½ï¼Œè¡¨ç¤ºç”Ÿæˆè¶Šå¿«ã€‚ğŸ† è¡¨ç¤ºè¯¥æŒ‡æ ‡æœ€ä½³æ¨¡å‹ï¼Œç²—ä½“è¡¨ç¤ºè·èƒœ*\n\n"
        return md

    def _generate_total_time_comparison(
        self,
        summaries: List[Dict[str, Any]],
        model_names: List[str],
        test_categories: List[str]
    ) -> str:
        """ç”Ÿæˆæ€»å“åº”æ—¶é—´å¯¹æ¯”è¡¨"""
        md = "| æµ‹è¯•ç±»åˆ« | " + " | ".join(model_names) + " |\n"
        md += "|---------|" + "|".join(["---------"] * len(model_names)) + "|\n"

        for category in test_categories:
            row = [category]

            # è·å–å„æ¨¡å‹çš„æ€»å“åº”æ—¶é—´
            model_times = {}
            for model in model_names:
                model_data = [s for s in summaries if s['category'] == category and s['model_name'] == model]
                if model_data and model_data[0]['test_count'] > 0:
                    total_time = model_data[0]['total_time_mean']
                    model_times[model] = total_time
                else:
                    model_times[model] = None

            # ä½¿ç”¨æ ¼å¼åŒ–æ–¹æ³•çªå‡ºè·èƒœåˆ†æ•°ï¼ˆæ€»æ—¶é—´è¶ŠçŸ­è¶Šå¥½ï¼ŒğŸ† ä¼šè‡ªåŠ¨æ·»åŠ åˆ°è·èƒœå€¼ï¼‰
            times = [t for t in model_times.values() if t is not None]
            for model in model_names:
                if model_times[model] is not None:
                    row.append(self._format_winning_score(model_times[model], times, is_lower_better=True))
                else:
                    row.append("N/A")

            md += "| " + " | ".join(row) + " |\n"

        md += "\n"
        md += "*æ³¨ï¼šæ€»å“åº”æ—¶é—´è¶ŠçŸ­è¶Šå¥½ï¼Œè¡¨ç¤ºå“åº”è¶Šå¿«ã€‚ğŸ† è¡¨ç¤ºè¯¥æŒ‡æ ‡æœ€ä½³æ¨¡å‹ï¼Œç²—ä½“è¡¨ç¤ºè·èƒœ*\n\n"
        return md

    def _generate_category_analysis(
        self,
        summaries: List[Dict[str, Any]],
        model_names: List[str],
        test_categories: List[str]
    ) -> str:
        """ç”Ÿæˆåˆ†åœºæ™¯åˆ†æ"""
        md = ""

        for category in test_categories:
            md += f"#### {category}\n\n"

            # è·å–è¯¥ç±»åˆ«çš„æ•°æ®
            category_data = {}
            for model in model_names:
                model_summaries = [s for s in summaries if s['category'] == category and s['model_name'] == model]
                if model_summaries and model_summaries[0]['test_count'] > 0:
                    category_data[model] = model_summaries[0]

            if len(category_data) < 2:
                md += "*æ•°æ®ä¸è¶³ï¼Œæ— æ³•å¯¹æ¯”*\n\n"
                continue

            # å¯¹æ¯”åˆ†æ
            model1, model2 = model_names[0], model_names[1]

            if model1 in category_data and model2 in category_data:
                data1 = category_data[model1]
                data2 = category_data[model2]

                # TTFT å¯¹æ¯”
                ttft_diff = data2['ttft_mean'] - data1['ttft_mean']
                ttft_better = model1 if ttft_diff > 0 else model2
                ttft_pct = abs(ttft_diff) / data2['ttft_mean'] * 100 if data2['ttft_mean'] > 0 else 0

                md += f"- **TTFT**: {ttft_better} é¢†å…ˆ {ttft_pct:.1f}% "
                md += f"({data1['ttft_mean']:.2f}ms vs {data2['ttft_mean']:.2f}ms)\n"

                # ç”Ÿæˆé€Ÿåº¦å¯¹æ¯”
                speed_diff = data1['speed_mean'] - data2['speed_mean']
                speed_better = model1 if speed_diff > 0 else model2
                speed_pct = abs(speed_diff) / data2['speed_mean'] * 100 if data2['speed_mean'] > 0 else 0

                md += f"- **ç”Ÿæˆé€Ÿåº¦**: {speed_better} é¢†å…ˆ {speed_pct:.1f}% "
                md += f"({data1['speed_mean']:.2f} vs {data2['speed_mean']:.2f} tokens/s)\n"

                # æ€»æ—¶é—´å¯¹æ¯”
                time_diff = data2['total_time_mean'] - data1['total_time_mean']
                time_better = model1 if time_diff > 0 else model2
                time_pct = abs(time_diff) / data2['total_time_mean'] * 100 if data2['total_time_mean'] > 0 else 0

                md += f"- **æ€»å“åº”æ—¶é—´**: {time_better} é¢†å…ˆ {time_pct:.1f}% "
                md += f"({data1['total_time_mean']:.2f}ms vs {data2['total_time_mean']:.2f}ms)\n\n"

        return md

    def _generate_model_advantages(
        self,
        summaries: List[Dict[str, Any]],
        model_names: List[str],
        test_categories: List[str]
    ) -> str:
        """ç”Ÿæˆæ¨¡å‹ä¼˜åŠ¿åˆ†æ"""
        md = ""

        # ç»Ÿè®¡å„æ¨¡å‹åœ¨å„ç±»åˆ«çš„ä¼˜åŠ¿æ¬¡æ•°
        advantages = {model: {'ttft': 0, 'speed': 0, 'time': 0} for model in model_names}

        for category in test_categories:
            category_data = {}
            for model in model_names:
                model_summaries = [s for s in summaries if s['category'] == category and s['model_name'] == model]
                if model_summaries and model_summaries[0]['test_count'] > 0:
                    category_data[model] = model_summaries[0]

            if len(category_data) == 2:
                model1, model2 = model_names[0], model_names[1]
                data1, data2 = category_data[model1], category_data[model2]

                # TTFT ä¼˜åŠ¿
                if data1['ttft_mean'] < data2['ttft_mean']:
                    advantages[model1]['ttft'] += 1
                else:
                    advantages[model2]['ttft'] += 1

                # ç”Ÿæˆé€Ÿåº¦ä¼˜åŠ¿
                if data1['speed_mean'] > data2['speed_mean']:
                    advantages[model1]['speed'] += 1
                else:
                    advantages[model2]['speed'] += 1

                # æ€»æ—¶é—´ä¼˜åŠ¿
                if data1['total_time_mean'] < data2['total_time_mean']:
                    advantages[model1]['time'] += 1
                else:
                    advantages[model2]['time'] += 1

        # ç”Ÿæˆæ€»ç»“
        md += "**å„æ¨¡å‹ä¼˜åŠ¿åœºæ™¯ç»Ÿè®¡**ï¼š\n\n"
        md += "| æ¨¡å‹ | TTFT ä¼˜åŠ¿ | ç”Ÿæˆé€Ÿåº¦ä¼˜åŠ¿ | æ€»æ—¶é—´ä¼˜åŠ¿ | æ€»è®¡ |\n"
        md += "|------|----------|-------------|----------|------|\n"

        # æ‰¾å‡ºæ€»ä½“ä¼˜èƒœè€…ç”¨äºæ ¼å¼åŒ–
        model_totals = {model: advantages[model]['ttft'] + advantages[model]['speed'] + advantages[model]['time'] for model in model_names}
        max_total = max(model_totals.values()) if model_totals else 0
        all_totals = list(model_totals.values())

        for model in model_names:
            adv = advantages[model]
            total = model_totals[model]
            # çªå‡ºè·èƒœæ¨¡å‹çš„åç§°å’Œæ€»åˆ†æ•°ï¼ˆğŸ† åœ¨è¾ƒé«˜çš„æ€»åˆ†æ—è¾¹ï¼‰
            model_formatted = self._format_winning_model(model, total == max_total)
            total_formatted = self._format_winning_score(total, all_totals, is_lower_better=False)
            md += f"| {model_formatted} | {adv['ttft']} | {adv['speed']} | {adv['time']} | {total_formatted} |\n"

        md += "\n"

        # ç»¼åˆåˆ†æ
        md += "**ç»¼åˆåˆ†æ**ï¼š\n\n"

        model1_adv = advantages[model_names[0]]['ttft'] + advantages[model_names[0]]['speed'] + advantages[model_names[0]]['time']
        model2_adv = advantages[model_names[1]]['ttft'] + advantages[model_names[1]]['speed'] + advantages[model_names[1]]['time']

        if model1_adv > model2_adv:
            md += f"- **{model_names[0]}** åœ¨ {model1_adv} ä¸ªåœºæ™¯ä¸­è¡¨ç°æ›´å¥½\n"
            md += f"- {model_names[1]} åœ¨ {model2_adv} ä¸ªåœºæ™¯ä¸­è¡¨ç°æ›´å¥½\n\n"
        elif model2_adv > model1_adv:
            md += f"- **{model_names[1]}** åœ¨ {model2_adv} ä¸ªåœºæ™¯ä¸­è¡¨ç°æ›´å¥½\n"
            md += f"- {model_names[0]} åœ¨ {model1_adv} ä¸ªåœºæ™¯ä¸­è¡¨ç°æ›´å¥½\n\n"
        else:
            md += "- ä¸¤ä¸ªæ¨¡å‹å„æœ‰ä¼˜åŠ¿ï¼Œè¡¨ç°ç›¸å½“\n\n"

        return md

    def _generate_recommendations(
        self,
        statistics: Dict[str, Any],
        summaries: List[Dict[str, Any]],
        model_names: List[str],
        quality_stats: Dict[str, Any] = None,
        minimax_stats: Dict[str, Any] = None
    ) -> str:
        """ç”Ÿæˆæ„è§å»ºè®®éƒ¨åˆ†"""
        # æ ¹æ®æ˜¯å¦æœ‰ MiniMax æ•°æ®å†³å®šç« èŠ‚æ ‡é¢˜ç¼–å·
        section_num = "å…­" if minimax_stats else "å››"
        md = f"## {section_num}ã€æ„è§å»ºè®®\n\n"

        # 4.1 æ€§èƒ½æ€»ç»“
        md += "### 4.1 æ€§èƒ½æ€»ç»“\n\n"

        # è®¡ç®—æ€»ä½“ç»Ÿè®¡
        overall_stats = {}
        for model in model_names:
            model_summaries = [s for s in summaries if s['model_name'] == model and s['test_count'] > 0]
            if model_summaries:
                overall_stats[model] = {
                    'avg_ttft': sum(s['ttft_mean'] for s in model_summaries) / len(model_summaries),
                    'avg_speed': sum(s['speed_mean'] for s in model_summaries) / len(model_summaries),
                    'avg_time': sum(s['total_time_mean'] for s in model_summaries) / len(model_summaries)
                }

        if len(overall_stats) == 2:
            model1, model2 = model_names[0], model_names[1]
            stats1 = overall_stats[model1]
            stats2 = overall_stats[model2]

            # TTFT å¯¹æ¯”
            if stats1['avg_ttft'] < stats2['avg_ttft']:
                md += f"- **é¦–æ¬¡å“åº”é€Ÿåº¦**: {model1} æ›´å¿«ï¼Œå¹³å‡å¿« {((stats2['avg_ttft'] - stats1['avg_ttft']) / stats2['avg_ttft'] * 100):.1f}%\n"
            else:
                md += f"- **é¦–æ¬¡å“åº”é€Ÿåº¦**: {model2} æ›´å¿«ï¼Œå¹³å‡å¿« {((stats1['avg_ttft'] - stats2['avg_ttft']) / stats1['avg_ttft'] * 100):.1f}%\n"

            # ç”Ÿæˆé€Ÿåº¦å¯¹æ¯”
            if stats1['avg_speed'] > stats2['avg_speed']:
                if stats2['avg_speed'] > 0:
                    md += f"- **ç”Ÿæˆé€Ÿåº¦**: {model1} æ›´å¿«ï¼Œå¹³å‡å¿« {((stats1['avg_speed'] - stats2['avg_speed']) / stats2['avg_speed'] * 100):.1f}%\n"
                else:
                    md += f"- **ç”Ÿæˆé€Ÿåº¦**: {model1} æ›´å¿« ({stats1['avg_speed']:.2f} vs {stats2['avg_speed']:.2f} tokens/s)\n"
            else:
                if stats1['avg_speed'] > 0:
                    md += f"- **ç”Ÿæˆé€Ÿåº¦**: {model2} æ›´å¿«ï¼Œå¹³å‡å¿« {((stats2['avg_speed'] - stats1['avg_speed']) / stats1['avg_speed'] * 100):.1f}%\n"
                else:
                    md += f"- **ç”Ÿæˆé€Ÿåº¦**: {model2} æ›´å¿« ({stats2['avg_speed']:.2f} vs {stats1['avg_speed']:.2f} tokens/s)\n"

            md += "\n"

        # 4.2 ä½¿ç”¨å»ºè®®
        md += "### 4.2 ä½¿ç”¨å»ºè®®\n\n"

        md += "**æ ¹æ®ä¸åŒåœºæ™¯çš„æ¨è**ï¼š\n\n"

        # ç®€å•é—®ç­”åœºæ™¯
        qa_data = [s for s in summaries if s['category'] == 'qa_simple']
        if qa_data:
            md += "**ç®€å•é—®ç­”åœºæ™¯**ï¼š\n\n"
            best_ttft = min(qa_data, key=lambda x: x['ttft_mean'])
            md += f"- å¦‚æœè¿½æ±‚å¿«é€Ÿå“åº”ï¼Œæ¨èä½¿ç”¨ **{best_ttft['model_name']}** (TTFT: {best_ttft['ttft_mean']:.2f}ms)\n\n"

        # é•¿æ–‡æœ¬ç”Ÿæˆåœºæ™¯
        long_data = [s for s in summaries if s['category'] == 'generation_long']
        if long_data:
            md += "**é•¿æ–‡æœ¬ç”Ÿæˆåœºæ™¯**ï¼š\n\n"
            best_speed = max(long_data, key=lambda x: x['speed_mean'])
            md += f"- å¦‚æœéœ€è¦ç”Ÿæˆå¤§é‡å†…å®¹ï¼Œæ¨èä½¿ç”¨ **{best_speed['model_name']}** (é€Ÿåº¦: {best_speed['speed_mean']:.2f} tokens/s)\n\n"

        # ä»£ç ç”Ÿæˆåœºæ™¯
        code_data = [s for s in summaries if s['category'] == 'code_generation']
        if code_data:
            md += "**ä»£ç ç”Ÿæˆåœºæ™¯**ï¼š\n\n"
            best_time = min(code_data, key=lambda x: x['total_time_mean'])
            md += f"- ä»£ç ç”Ÿæˆä»»åŠ¡æ¨èä½¿ç”¨ **{best_time['model_name']}** (æ€»æ—¶é—´: {best_time['total_time_mean']:.2f}ms)\n\n"

        # 4.3 ä¼˜åŒ–å»ºè®®
        md += "### 4.3 ä¼˜åŒ–å»ºè®®\n\n"

        md += "**å¯¹äº API ä½¿ç”¨æ–¹**ï¼š\n\n"
        md += "1. **é¢„çƒ­è¿æ¥**: åœ¨æ­£å¼è¯·æ±‚å‰è¿›è¡Œ 1-2 æ¬¡é¢„çƒ­è¯·æ±‚ï¼Œå¯ä»¥æ˜¾è‘—æ”¹å–„é¦–æ¬¡å“åº”æ—¶é—´\n"
        md += "2. **æµå¼å“åº”**: å¯¹äºé•¿æ–‡æœ¬ç”Ÿæˆï¼ŒåŠ¡å¿…ä½¿ç”¨æµå¼å“åº”æ¥å£ï¼Œå¯ä»¥æ”¹å–„ç”¨æˆ·ä½“éªŒ\n"
        md += "3. **æ¨¡å‹é€‰æ‹©**: æ ¹æ®å…·ä½“åœºæ™¯é€‰æ‹©åˆé€‚çš„æ¨¡å‹ï¼Œç®€å•é—®ç­”ä¼˜å…ˆè€ƒè™‘ TTFTï¼Œé•¿æ–‡æœ¬ç”Ÿæˆä¼˜å…ˆè€ƒè™‘ç”Ÿæˆé€Ÿåº¦\n\n"

        md += "**å¯¹äº API æä¾›æ–¹**ï¼š\n\n"
        md += "1. **æŒç»­ä¼˜åŒ– TTFT**: é¦–æ¬¡å“åº”æ—¶é—´æ˜¯ç”¨æˆ·ä½“éªŒçš„å…³é”®æŒ‡æ ‡\n"
        md += "2. **æå‡ç”Ÿæˆé€Ÿåº¦**: ç‰¹åˆ«æ˜¯å¯¹äºé•¿æ–‡æœ¬ç”Ÿæˆåœºæ™¯ï¼Œç”Ÿæˆé€Ÿåº¦ç›´æ¥å½±å“ç”¨æˆ·ç­‰å¾…æ—¶é—´\n"
        md += "3. **ä¿æŒç¨³å®šæ€§**: å‡å°‘å“åº”æ—¶é—´çš„æ³¢åŠ¨ï¼Œæå‡ç”¨æˆ·ä½“éªŒçš„ä¸€è‡´æ€§\n\n"

        md += "---\n\n"
        return md

    def _generate_quality_assessment(
        self,
        quality_stats: Dict[str, Any],
        model_names: List[str],
        test_categories: List[str],
        raw_results: List[Dict[str, Any]],
        has_minimax: bool = False
    ) -> str:
        """ç”Ÿæˆè´¨é‡è¯„ä¼°ç« èŠ‚"""
        # æ ¹æ®æ˜¯å¦æœ‰ MiniMax æ•°æ®å†³å®šç« èŠ‚ç¼–å·
        section_num = "äº”" if has_minimax else "å››"
        md = f"## {section_num}ã€è´¨é‡è¯„ä¼°\n\n"

        # 4.1 è¯„ä¼°æ¦‚è¿°
        md += "### 4.1 è¯„ä¼°æ¦‚è¿°\n\n"
        md += f"æœ¬æ¬¡æµ‹è¯•ä½¿ç”¨ {len(quality_stats.get('by_judge', {}))} ä¸ª Judge æ¨¡å‹è¿›è¡Œè´¨é‡è¯„ä¼°ï¼š\n\n"

        for judge_name, judge_stats in quality_stats.get("by_judge", {}).items():
            md += f"- **{judge_name}** (æ¨¡å‹: {judge_stats.get('judge_model', 'N/A')})\n"
            md += f"  - è¯„ä¼°æ¬¡æ•°: {judge_stats.get('total_evaluations', 0)}\n"
            md += f"  - å¹³å‡åˆ†æ•°: {judge_stats.get('avg_score', 0):.2f}/5.0\n\n"

        # 4.2 å„ Judge è¯„ä¼°ç»“æœ
        md += "### 4.2 å„ Judge è¯„ä¼°ç»“æœ\n\n"

        for judge_name in quality_stats.get("by_judge", {}).keys():
            md += f"#### {judge_name} çš„è¯„ä¼°\n\n"
            md += self._generate_judge_results(
                judge_name, quality_stats, model_names, test_categories
            )

        # 4.3 Judge é—´è¯„ä¼°å¯¹æ¯”
        md += "### 4.3 Judge é—´è¯„ä¼°å¯¹æ¯”\n\n"
        md += self._generate_judge_comparison(quality_stats, model_names, test_categories)

        # 4.4 è´¨é‡è¯„ä¼°ç¤ºä¾‹
        md += "### 4.4 è´¨é‡è¯„ä¼°ç¤ºä¾‹\n\n"
        md += self._generate_quality_examples(raw_results, model_names)

        md += "---\n\n"
        return md

    def _generate_judge_results(
        self,
        judge_name: str,
        quality_stats: Dict[str, Any],
        model_names: List[str],
        test_categories: List[str]
    ) -> str:
        """ç”Ÿæˆå•ä¸ª Judge çš„è¯„ä¼°ç»“æœ"""
        md = ""

        # æŒ‰æ¨¡å‹ç»Ÿè®¡
        md += "**æŒ‰æ¨¡å‹ç»Ÿè®¡**ï¼š\n\n"
        md += "| æ¨¡å‹ | å¹³å‡åˆ†æ•° | è¯„ä¼°æ¬¡æ•° |\n"
        md += "|------|---------|----------|\n"

        for model_name in model_names:
            if model_name in quality_stats.get("by_model", {}):
                model_stats = quality_stats["by_model"][model_name]
                if judge_name in model_stats:
                    stats = model_stats[judge_name]
                    md += f"| {model_name} | {stats['avg_score']:.2f} | {stats['total_evaluations']} |\n"

        md += "\n"

        # æŒ‰ç±»åˆ«ç»Ÿè®¡
        md += "**æŒ‰ç±»åˆ«ç»Ÿè®¡**ï¼š\n\n"
        md += "| ç±»åˆ« | " + " | ".join(model_names) + " |\n"
        md += "|------|" + "|".join(["---------"] * len(model_names)) + "|\n"

        for category in test_categories:
            if category in quality_stats.get("by_category", {}):
                row = [category]
                for model_name in model_names:
                    if model_name in quality_stats["by_category"][category]:
                        model_stats = quality_stats["by_category"][category][model_name]
                        if judge_name in model_stats:
                            row.append(f"{model_stats[judge_name]['avg_score']:.2f}")
                        else:
                            row.append("N/A")
                    else:
                        row.append("N/A")
                md += "| " + " | ".join(row) + " |\n"

        md += "\n"
        return md

    def _generate_judge_comparison(
        self,
        quality_stats: Dict[str, Any],
        model_names: List[str],
        test_categories: List[str]
    ) -> str:
        """ç”Ÿæˆ Judge é—´å¯¹æ¯”"""
        md = "**Judge è¯„ä¼°ä¸€è‡´æ€§åˆ†æ**ï¼š\n\n"

        judges = list(quality_stats.get("by_judge", {}).keys())
        if len(judges) < 2:
            md += "*åªæœ‰ä¸€ä¸ª Judgeï¼Œæ— æ³•è¿›è¡Œå¯¹æ¯”*\n\n"
            return md

        # æŒ‰ç±»åˆ«å¯¹æ¯”
        md += "| æµ‹è¯•ç±»åˆ« | " + " | ".join([f"{j} è¯„ä¼° {m}" for j in judges for m in model_names]) + " |\n"
        md += "|---------|" + "|".join(["------------------"] * (len(judges) * len(model_names))) + "|\n"

        for category in test_categories:
            if category not in quality_stats.get("by_category", {}):
                continue

            row = [category]
            category_stats = quality_stats["by_category"][category]

            for judge_name in judges:
                for model_name in model_names:
                    if model_name in category_stats and judge_name in category_stats[model_name]:
                        score = category_stats[model_name][judge_name]['avg_score']
                        row.append(f"{score:.2f}")
                    else:
                        row.append("N/A")

            md += "| " + " | ".join(row) + " |\n"

        md += "\n"

        # è®¡ç®—åˆ†æ•°å·®å¼‚
        md += "**Judge é—´åˆ†æ•°å·®å¼‚**ï¼š\n\n"
        md += "| æ¨¡å‹ | å¹³å‡åˆ†æ•°å·®å¼‚ |\n"
        md += "|------|-------------|\n"

        for model_name in model_names:
            if model_name in quality_stats.get("by_model", {}):
                model_stats = quality_stats["by_model"][model_name]
                scores = [judge_stats['avg_score'] for judge_stats in model_stats.values()]
                if len(scores) >= 2:
                    diff = max(scores) - min(scores)
                    md += f"| {model_name} | {diff:.2f} |\n"

        md += "\n"
        return md

    def _generate_quality_examples(
        self,
        raw_results: List[Dict[str, Any]],
        model_names: List[str]
    ) -> str:
        """ç”Ÿæˆè´¨é‡è¯„ä¼°ç¤ºä¾‹"""
        md = "**å…¸å‹è¯„ä¼°æ¡ˆä¾‹**ï¼š\n\n"

        # æ‰¾å‡ºæœ‰è´¨é‡è¯„ä¼°çš„ç»“æœ
        quality_results = [r for r in raw_results if r.get("quality_evaluations")]

        if not quality_results:
            md += "*æš‚æ— è¯¦ç»†è¯„ä¼°æ¡ˆä¾‹*\n\n"
            return md

        # å±•ç¤ºå‰ 3 ä¸ªæ¡ˆä¾‹
        for i, result in enumerate(quality_results[:3], 1):
            md += f"#### æ¡ˆä¾‹ {i}: {result.get('test_name', 'Unknown')}\n\n"
            md += f"- **æµ‹è¯•ç±»åˆ«**: {result.get('test_category', 'N/A')}\n"
            md += f"- **è¢«è¯„ä¼°æ¨¡å‹**: {result.get('model_name', 'N/A')}\n\n"

            # å±•ç¤ºå„ Judge çš„è¯„ä¼°
            for judge_name, evaluation in result.get("quality_evaluations", {}).items():
                if not evaluation.get("success", False):
                    continue

                md += f"**{judge_name} è¯„ä¼°**ï¼š\n\n"
                md += f"- æ€»ä½“åˆ†æ•°: {evaluation.get('overall_score', 0):.2f}/5.0\n"

                # å±•ç¤ºå„æ ‡å‡†åˆ†æ•°
                scores = evaluation.get('scores', {})
                if scores:
                    md += "- å„é¡¹åˆ†æ•°:\n"
                    for criterion, score in scores.items():
                        md += f"  - {criterion}: {score:.1f}\n"

                # å±•ç¤ºä¼˜ç¼ºç‚¹
                if evaluation.get('strengths'):
                    md += f"- ä¼˜ç‚¹: {', '.join(evaluation['strengths'][:3])}\n"

                if evaluation.get('weaknesses'):
                    md += f"- ç¼ºç‚¹: {', '.join(evaluation['weaknesses'][:3])}\n"

                # å±•ç¤ºè¯„ä¼°ç†ç”±ï¼ˆæˆªæ–­ï¼‰
                reasoning = evaluation.get('reasoning', '')
                if reasoning:
                    reasoning_short = reasoning[:200] + "..." if len(reasoning) > 200 else reasoning
                    md += f"- è¯„ä¼°ç†ç”±: {reasoning_short}\n"

                md += "\n"

            md += "\n"

        return md

    def _generate_footer(self) -> str:
        """ç”ŸæˆæŠ¥å‘Šå°¾éƒ¨"""
        return f"""---

## é™„å½•

### æµ‹è¯•é…ç½®

- **é¢„çƒ­è¿è¡Œæ¬¡æ•°**: 2 æ¬¡
- **æ­£å¼æµ‹è¯•æ¬¡æ•°**: æ¯ä¸ªæµ‹è¯• 3 æ¬¡
- **è®¡æ—¶ç²¾åº¦**: çº³ç§’çº§ (ä½¿ç”¨ `time.perf_counter()`)
- **API åè®®**: Anthropic Messages API (æµå¼å“åº”)

### æ•°æ®è¯´æ˜

- æ‰€æœ‰æµ‹è¯•æ•°æ®å‡ä¸ºå¤šæ¬¡è¿è¡Œçš„å¹³å‡å€¼
- æµ‹è¯•ç»“æœå¯èƒ½å—ç½‘ç»œæ¡ä»¶ã€æœåŠ¡å™¨è´Ÿè½½ç­‰å› ç´ å½±å“
- å»ºè®®åœ¨ä¸åŒæ—¶é—´æ®µå¤šæ¬¡æµ‹è¯•ä»¥è·å¾—æ›´å‡†ç¡®çš„ç»“æœ

### æŠ¥å‘Šè¯´æ˜

æœ¬æŠ¥å‘Šç”±è‡ªåŠ¨åŒ–æµ‹è¯•ç³»ç»Ÿç”Ÿæˆï¼Œæµ‹è¯•æ—¶é—´æˆ³ï¼š{self.timestamp}

å®Œæ•´åŸå§‹æ•°æ®å·²ä¿å­˜è‡³ `results/data/raw_data_{self.timestamp}.json`

---

*æŠ¥å‘Šç”Ÿæˆå·¥å…·: DeepSeek vs GLM API åŸºå‡†æµ‹è¯•ç³»ç»Ÿ v1.0*
"""
