"""ç»Ÿä¸€è¯„æµ‹è„šæœ¬ - æ”¯æŒ MiniMax æ ‡å‡†è¯„æµ‹å’ŒåŸå§‹å¯¹æ¯”æµ‹è¯•

æ”¯æŒå››ç§è¿è¡Œæ¨¡å¼ï¼š
- --mode standard: è¿è¡Œå®Œæ•´ MiniMax æ ‡å‡†è¯„æµ‹ï¼ˆ100ä¸ªç”¨ä¾‹ï¼‰
  âœ… ä¿å­˜JSON
  âœ… ç”ŸæˆMarkdownæŠ¥å‘Š
  âœ… ç”ŸæˆHTMLæŠ¥å‘Š

- --mode preview: è¿è¡Œé¢„æµ‹è¯•ï¼ˆæ¯ä¸ªç»´åº¦1ä¸ªç”¨ä¾‹ï¼Œå…±4ä¸ªï¼‰
  âœ… ä¿å­˜JSON
  âœ… ç”ŸæˆMarkdownæŠ¥å‘Š
  âœ… ç”ŸæˆHTMLæŠ¥å‘Š

- --mode single: è¿è¡Œå•ç”¨ä¾‹æµ‹è¯•ï¼ˆ1ä¸ªç”¨ä¾‹ï¼‰
  âœ… ä¿å­˜JSON
  âœ… ç”ŸæˆMarkdownæŠ¥å‘Š
  âœ… ç”ŸæˆHTMLæŠ¥å‘Š

- --mode original: è¿è¡ŒåŸå§‹å¯¹æ¯”æµ‹è¯•ï¼ˆæ¯ç±»åˆ«1ä¸ªç”¨ä¾‹ï¼Œå…±4ä¸ªï¼‰
  âœ… ä¿å­˜JSON
  âœ… ç”ŸæˆMarkdownæŠ¥å‘Š
  âŒ ä¸ç”ŸæˆHTMLæŠ¥å‘Šï¼ˆç®€åŒ–æ¨¡å¼ï¼‰

ä½¿ç”¨ç¤ºä¾‹ï¼š
    python run_benchmark.py                    # é»˜è®¤ï¼šæ ‡å‡†æ¨¡å¼
    python run_benchmark.py --mode preview     # é¢„æµ‹è¯•æ¨¡å¼
    python run_benchmark.py --mode single      # å•ç”¨ä¾‹æµ‹è¯•
    python run_benchmark.py --mode original    # åŸå§‹å¯¹æ¯”æµ‹è¯•æ¨¡å¼
"""

import sys
import os
import io
import argparse
from pathlib import Path
from datetime import datetime
from collections import Counter

# è®¾ç½® UTF-8 ç¼–ç ï¼ˆWindows å…¼å®¹ï¼‰
if sys.platform == 'win32':
    sys.stdout = io.TextIOWrapper(sys.stdout.buffer, encoding='utf-8')
    sys.stderr = io.TextIOWrapper(sys.stderr.buffer, encoding='utf-8')

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ° Python è·¯å¾„
project_root = Path(__file__).parent
sys.path.insert(0, str(project_root))

from dotenv import load_dotenv

# åŠ è½½ .env æ–‡ä»¶
load_dotenv()

# éªŒè¯ MINIMAX_API_KEY å·²è®¾ç½®
if not os.environ.get("MINIMAX_API_KEY"):
    print("è­¦å‘Š: æœªè®¾ç½® MINIMAX_API_KEY ç¯å¢ƒå˜é‡")
    print("è¯·åœ¨ .env æ–‡ä»¶ä¸­è®¾ç½®: MINIMAX_API_KEY=your_api_key_here")
    print("MiniMax Judge å°†æ— æ³•å‚ä¸è´¨é‡è¯„ä¼°ã€‚\n")

from src.utils.config_loader import load_config, validate_config
from src.benchmark.runner import BenchmarkRunner
from src.quality.judge_manager import JudgeManager
from src.quality.minimax_scorer import MiniMaxScoreCalculator
from src.report.minimax_generator import MiniMaxReportGenerator
from src.report.generator import ReportGenerator
from src.utils.json_saver import BenchmarkJSONSaver
from src.report.markdown_generator import MarkdownReportGenerator


# ============ MiniMax æ ‡å‡†è¯„æµ‹æ¨¡å¼ ============

def run_minimax_standard(config):
    """è¿è¡Œå®Œæ•´çš„ MiniMax æ ‡å‡†è¯„æµ‹ï¼ˆ100ä¸ªç”¨ä¾‹ï¼‰"""
    from src.tests.cases_minimax import (
        BasicPerformanceTests,
        CoreCapabilitiesTests,
        PracticalScenariosTests,
        AdvancedFeaturesTests
    )
    from src.tests.minimax_registry import minimax_registry

    print("=" * 80)
    print("DeepSeek vs GLM - MiniMax æ ‡å‡†è¯„æµ‹ï¼ˆ100ä¸ªç”¨ä¾‹ï¼‰")
    print(f"å¼€å§‹æ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
    print("=" * 80)
    print()

    # åŠ è½½é…ç½®
    print("åŠ è½½é…ç½®æ–‡ä»¶...")
    if not validate_config(config):
        print("âŒ é…ç½®éªŒè¯å¤±è´¥")
        sys.exit(1)
    print("âœ“ é…ç½®åŠ è½½æˆåŠŸ\n")

    # MiniMax Judge æå‰éªŒè¯
    quality_config = config.get("quality", {})
    if quality_config.get("enabled", False):
        validate_minimax_judge(config)

    # è·å–æ‰€æœ‰æµ‹è¯•ç”¨ä¾‹
    all_test_cases = minimax_registry.get_all_test_cases()
    print(f"âœ“ åŠ è½½ MiniMax æ ‡å‡†æµ‹è¯•ç”¨ä¾‹: {len(all_test_cases)} ä¸ª")

    dimension_counts = Counter(tc.dimension for tc in all_test_cases)
    print("\næµ‹è¯•ç”¨ä¾‹åˆ†å¸ƒ:")
    for dimension, count in dimension_counts.items():
        print(f"  - {dimension}: {count} ä¸ª")
    print()

    # é˜¶æ®µæ€§ä¿å­˜é…ç½® - æš‚æ—¶ç¦ç”¨å¢é‡ä¿å­˜ï¼ˆå­˜åœ¨é™¤é›¶é”™è¯¯bugï¼‰
    incremental_saves = []  # ç¦ç”¨å¢é‡ä¿å­˜ï¼Œåªåœ¨æœ€åä¿å­˜å®Œæ•´ç»“æœ

    # åˆ›å»ºè¿›åº¦å›è°ƒå‡½æ•° - åªæ˜¾ç¤ºè¿›åº¦é‡Œç¨‹ç¢‘
    def progress_callback(completed, total, metrics_collector):
        # æ¯10ä¸ªæµ‹è¯•ç”¨ä¾‹æ˜¾ç¤ºä¸€æ¬¡è¿›åº¦
        if completed % 10 == 0 and completed > 0:
            statistics = metrics_collector.get_statistics()
            model_names = list(statistics.get('model_stats', {}).keys())

            print(f"\n{'=' * 80}")
            print(f"ğŸ“Š è¿›åº¦é‡Œç¨‹ç¢‘: å·²å®Œæˆ {completed}/{total} ä¸ªæµ‹è¯•ç”¨ä¾‹")
            print(f"{'=' * 80}")
            print(f"  - æˆåŠŸç‡: {statistics['success_rate']*100:.1f}%")
            for model_name in model_names:
                model_stats = statistics['model_stats'][model_name]
                print(f"  - {model_name}: {model_stats['success']}/{model_stats['total']} æµ‹è¯•æˆåŠŸ")
            print()

    # è¿è¡ŒåŸºå‡†æµ‹è¯•
    runner = BenchmarkRunner(config)
    print("\néªŒè¯ API è¿æ¥...")
    connection_status = runner.validate_connections()
    for api_name, status in connection_status.items():
        print(f"{api_name}: {'âœ“ æˆåŠŸ' if status else 'âœ— å¤±è´¥'}")
    print()

    # æ‰§è¡Œæµ‹è¯•
    print("\n" + "=" * 80)
    print("å¼€å§‹åŸºå‡†æµ‹è¯•")
    print("=" * 80 + "\n")

    metrics_collector = runner.run_benchmark(
        test_cases=[tc.to_dict() for tc in all_test_cases],
        show_progress=True,
        progress_callback=progress_callback
    )

    # è·å–ç»“æœ
    statistics = metrics_collector.get_statistics()
    print(f"\næµ‹è¯•å®Œæˆ:")
    print(f"  - æ€»æµ‹è¯•æ•°: {statistics['total_tests']}")
    print(f"  - æˆåŠŸ: {statistics['successful_tests']}")
    print(f"  - å¤±è´¥: {statistics['failed_tests']}")
    print(f"  - æˆåŠŸç‡: {statistics['success_rate']*100:.1f}%")

    # è®¡ç®—ç»´åº¦å¾—åˆ†
    model_names = runner.get_model_names()
    print(f"\nå¯¹æ¯”çš„æ¨¡å‹: {', '.join(model_names)}")

    dimension_weights = {
        "basic_performance": 0.25,
        "core_capabilities": 0.35,
        "practical_scenarios": 0.25,
        "advanced_features": 0.15
    }

    categories = list(dimension_counts.keys())
    summaries = metrics_collector.calculate_all_summaries(
        model_names=model_names,
        categories=categories
    )

    # ä¸‰æ¨¡å‹äº¤å‰è¯„ä»·
    if quality_config.get("enabled", False):
        print("\n" + "=" * 80)
        print("å¼€å§‹ä¸‰æ¨¡å‹äº¤å‰è¯„ä»·...")
        print("=" * 80 + "\n")

        judge_manager = JudgeManager(config)
        scorer = MiniMaxScoreCalculator()
        quality_stats = metrics_collector.get_quality_statistics()
        print(f"è´¨é‡è¯„ä¼°å®Œæˆ:")
        print(f"  - æ€»è¯„ä¼°æ•°: {quality_stats['overall']['total_evaluations']}")
        print(f"  - æˆåŠŸè¯„ä¼°: {quality_stats['overall']['successful_evaluations']}")
    else:
        scorer = None
        quality_stats = None

    # è®¡ç®—ç»´åº¦å¾—åˆ†
    print("\nè®¡ç®— MiniMax æ ‡å‡†ç»´åº¦å¾—åˆ†...")
    quality_scores = calculate_dimension_scores(model_names, summaries, dimension_weights, metrics_collector)

    # ä¿å­˜è¯„æµ‹æ•°æ®åˆ°JSON
    print("\n" + "=" * 80)
    print("ä¿å­˜è¯„æµ‹æ•°æ®åˆ°JSON...")
    print("=" * 80)
    raw_results = metrics_collector.export_results()
    json_saver = BenchmarkJSONSaver()
    json_path = json_saver.save_evaluation_data(
        statistics=statistics,
        quality_scores=quality_scores,
        summaries=[s.to_dict() for s in summaries],
        raw_results=raw_results,
        dimension_weights=dimension_weights,
        quality_evaluations=metrics_collector.get_quality_statistics() if quality_config.get("enabled", False) else {},
        config=config
    )
    print(f"âœ… JSONæ•°æ®å·²ä¿å­˜: {json_path}")

    # ä»JSONç”ŸæˆMarkdownæŠ¥å‘Š
    print("\n" + "=" * 80)
    print("ä»JSONç”ŸæˆMarkdownæŠ¥å‘Š...")
    print("=" * 80)
    md_generator = MarkdownReportGenerator()
    md_path = md_generator.generate_from_json(json_path)
    print(f"âœ… MarkdownæŠ¥å‘Šå·²ç”Ÿæˆ: {md_path}")

    # ç”ŸæˆHTMLæŠ¥å‘Šï¼ˆä¿ç•™åŸæœ‰åŠŸèƒ½ï¼‰
    print("\n" + "=" * 80)
    print("ç”ŸæˆHTMLæŠ¥å‘Š...")
    print("=" * 80 + "\n")

    report_generator = MiniMaxReportGenerator(config.get("report", {}))

    report_path = report_generator.generate_minimax_report(
        statistics=statistics,
        quality_scores=quality_scores,
        performance_data=raw_results,
        model_names=model_names,
        dimension_weights=dimension_weights
    )

    print(f"âœ… æŠ¥å‘Šå·²ç”Ÿæˆ: {report_path}")

    # æ‰“å°æ€»ç»“
    print("\n" + "=" * 80)
    print(f"âœ… MiniMax æ ‡å‡†è¯„æµ‹å®Œæˆï¼")
    print(f"ç»“æŸæ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
    print("=" * 80 + "\n")

    print("è¯„æµ‹æ€»ç»“:")
    for model_name in model_names:
        overall_score = quality_scores[model_name]["overall_score"]
        grade = scorer.grade_score(overall_score) if scorer else "N/A"
        print(f"\n{model_name}:")
        print(f"  ç»¼åˆå¾—åˆ†: {overall_score:.2f}/10")
        print(f"  ç­‰çº§: {grade}")

    print(f"\næŠ¥å‘Šè·¯å¾„: {report_path}")


def run_minimax_preview(config):
    """è¿è¡Œé¢„æµ‹è¯•ï¼ˆæ¯ä¸ªç»´åº¦1ä¸ªç”¨ä¾‹ï¼Œå…±4ä¸ªï¼‰"""
    from src.tests.cases_minimax import (
        BasicPerformanceTests,
        CoreCapabilitiesTests,
        PracticalScenariosTests,
        AdvancedFeaturesTests
    )
    from src.tests.minimax_registry import minimax_registry

    print("=" * 80)
    print("MiniMax æ ‡å‡†è¯„æµ‹ - é¢„æµ‹è¯•ï¼ˆæ¯ä¸ªç»´åº¦1ä¸ªç”¨ä¾‹ï¼‰")
    print(f"å¼€å§‹æ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
    print("=" * 80)
    print()

    # åŠ è½½é…ç½®
    if not validate_config(config):
        print("âŒ é…ç½®éªŒè¯å¤±è´¥")
        sys.exit(1)
    print("âœ“ é…ç½®åŠ è½½æˆåŠŸ\n")

    # MiniMax Judge éªŒè¯
    quality_config = config.get("quality", {})
    if quality_config.get("enabled", False):
        validate_minimax_judge(config)

    # è·å–æµ‹è¯•ç”¨ä¾‹
    all_test_cases = minimax_registry.get_all_test_cases()

    # é€‰æ‹©æ¯ä¸ªç»´åº¦çš„ç¬¬ä¸€ä¸ªç”¨ä¾‹
    selected_tests = []
    dimensions = ["basic_performance", "core_capabilities", "practical_scenarios", "advanced_features"]
    for dimension in dimensions:
        for tc in all_test_cases:
            if tc.dimension == dimension:
                selected_tests.append(tc)
                break

    print(f"âœ“ åŠ è½½é¢„æµ‹è¯•ç”¨ä¾‹: {len(selected_tests)} ä¸ªï¼ˆæ¯ä¸ªç»´åº¦1ä¸ªï¼‰")
    for tc in selected_tests:
        print(f"  - {tc.minimax_id}: {tc.sub_dimension}")
    print()

    # è¿è¡Œæµ‹è¯•
    runner = BenchmarkRunner(config)
    metrics_collector = runner.run_benchmark(
        test_cases=[tc.to_dict() for tc in selected_tests],
        show_progress=True
    )

    statistics = metrics_collector.get_statistics()
    print(f"\né¢„æµ‹è¯•å®Œæˆ:")
    print(f"  - æ€»æµ‹è¯•æ•°: {statistics['total_tests']}")
    print(f"  - æˆåŠŸ: {statistics['successful_tests']}")
    print(f"  - æˆåŠŸç‡: {statistics['success_rate']*100:.1f}%")

    # è®¡ç®—ç»´åº¦å¾—åˆ†
    model_names = runner.get_model_names()
    print(f"\nå¯¹æ¯”çš„æ¨¡å‹: {', '.join(model_names)}")

    dimension_weights = {
        "basic_performance": 0.25,
        "core_capabilities": 0.35,
        "practical_scenarios": 0.25,
        "advanced_features": 0.15
    }

    categories = list(dimension_weights.keys())
    summaries = metrics_collector.calculate_all_summaries(
        model_names=model_names,
        categories=categories
    )

    # ä¸‰æ¨¡å‹äº¤å‰è¯„ä»·ï¼ˆå¦‚æœå¯ç”¨ï¼‰
    if quality_config.get("enabled", False):
        print("\n" + "=" * 80)
        print("å¼€å§‹ä¸‰æ¨¡å‹äº¤å‰è¯„ä»·...")
        print("=" * 80 + "\n")

        from src.quality.judge_manager import JudgeManager
        from src.quality.minimax_scorer import MiniMaxScoreCalculator

        judge_manager = JudgeManager(config)
        scorer = MiniMaxScoreCalculator()
        quality_stats = metrics_collector.get_quality_statistics()
        print(f"è´¨é‡è¯„ä¼°å®Œæˆ:")
        print(f"  - æ€»è¯„ä¼°æ•°: {quality_stats['overall']['total_evaluations']}")
        print(f"  - æˆåŠŸè¯„ä¼°: {quality_stats['overall']['successful_evaluations']}")
    else:
        scorer = None
        quality_stats = None

    # è®¡ç®—ç»´åº¦å¾—åˆ†
    print("\nè®¡ç®— MiniMax æ ‡å‡†ç»´åº¦å¾—åˆ†...")
    quality_scores = calculate_dimension_scores(model_names, summaries, dimension_weights, metrics_collector)

    # ä¿å­˜è¯„æµ‹æ•°æ®åˆ°JSON
    print("\n" + "=" * 80)
    print("ä¿å­˜è¯„æµ‹æ•°æ®åˆ°JSON...")
    print("=" * 80)
    raw_results = metrics_collector.export_results()
    json_saver = BenchmarkJSONSaver()
    json_path = json_saver.save_evaluation_data(
        statistics=statistics,
        quality_scores=quality_scores,
        summaries=[s.to_dict() for s in summaries],
        raw_results=raw_results,
        dimension_weights=dimension_weights,
        quality_evaluations=metrics_collector.get_quality_statistics() if quality_config.get("enabled", False) else {},
        config=config
    )
    print(f"âœ… JSONæ•°æ®å·²ä¿å­˜: {json_path}")

    # ä»JSONç”ŸæˆMarkdownæŠ¥å‘Š
    print("\n" + "=" * 80)
    print("ä»JSONç”ŸæˆMarkdownæŠ¥å‘Š...")
    print("=" * 80)
    md_generator = MarkdownReportGenerator()
    md_path = md_generator.generate_from_json(json_path)
    print(f"âœ… MarkdownæŠ¥å‘Šå·²ç”Ÿæˆ: {md_path}")

    # ç”ŸæˆHTMLæŠ¥å‘Šï¼ˆä¿ç•™åŸæœ‰åŠŸèƒ½ï¼‰
    print("\n" + "=" * 80)
    print("ç”ŸæˆHTMLæŠ¥å‘Š...")
    print("=" * 80 + "\n")

    from src.report.minimax_generator import MiniMaxReportGenerator
    report_generator = MiniMaxReportGenerator(config.get("report", {}))

    report_path = report_generator.generate_minimax_report(
        statistics=statistics,
        quality_scores=quality_scores,
        performance_data=raw_results,
        model_names=model_names,
        dimension_weights=dimension_weights
    )

    print(f"âœ… æŠ¥å‘Šå·²ç”Ÿæˆ: {report_path}")

    # æ‰“å°æ€»ç»“
    print("\n" + "=" * 80)
    print("é¢„æµ‹è¯•è¯„æµ‹æ€»ç»“:")
    print("=" * 80)
    for model_name in model_names:
        overall_score = quality_scores[model_name]["overall_score"]
        grade = scorer.grade_score(overall_score) if scorer else "N/A"
        print(f"\n{model_name}:")
        print(f"  ç»¼åˆå¾—åˆ†: {overall_score:.2f}/10")
        print(f"  ç­‰çº§: {grade}")

    print("\n" + "=" * 80)
    print(f"âœ… é¢„æµ‹è¯•å®Œæˆï¼")
    print(f"ç»“æŸæ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
    print("=" * 80)


def run_single_test(config):
    """è¿è¡Œå•ç”¨ä¾‹æµ‹è¯•ï¼ˆåªè¿è¡Œç¬¬ä¸€ä¸ªæµ‹è¯•ç”¨ä¾‹ï¼‰"""
    from src.tests.cases_minimax import BasicPerformanceTests
    from src.tests.minimax_registry import minimax_registry

    print("=" * 80)
    print("MiniMax æ ‡å‡†è¯„æµ‹ - å•ç”¨ä¾‹æµ‹è¯•")
    print(f"å¼€å§‹æ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
    print("=" * 80)
    print()

    # åŠ è½½é…ç½®
    if not validate_config(config):
        print("âŒ é…ç½®éªŒè¯å¤±è´¥")
        sys.exit(1)
    print("âœ“ é…ç½®åŠ è½½æˆåŠŸ\n")

    # MiniMax Judge éªŒè¯
    quality_config = config.get("quality", {})
    if quality_config.get("enabled", False):
        validate_minimax_judge(config)

    # åªé€‰æ‹©ç¬¬ä¸€ä¸ªæµ‹è¯•ç”¨ä¾‹
    all_test_cases = minimax_registry.get_all_test_cases()
    if not all_test_cases:
        print("âŒ æœªæ‰¾åˆ°æµ‹è¯•ç”¨ä¾‹")
        sys.exit(1)

    selected_tests = [all_test_cases[0]]  # åªå–ç¬¬ä¸€ä¸ª

    print(f"âœ“ åŠ è½½å•ç”¨ä¾‹æµ‹è¯•: 1 ä¸ª")
    print(f"  - {selected_tests[0].minimax_id}: {selected_tests[0].sub_dimension}")
    print(f"  - æç¤ºè¯: {selected_tests[0].prompt[:100]}...")
    print()

    # è¿è¡Œæµ‹è¯•
    runner = BenchmarkRunner(config)
    metrics_collector = runner.run_benchmark(
        test_cases=[tc.to_dict() for tc in selected_tests],
        show_progress=True
    )

    statistics = metrics_collector.get_statistics()
    print(f"\nå•ç”¨ä¾‹æµ‹è¯•å®Œæˆ:")
    print(f"  - æ€»æµ‹è¯•æ•°: {statistics['total_tests']}")
    print(f"  - æˆåŠŸ: {statistics['successful_tests']}")
    print(f"  - æˆåŠŸç‡: {statistics['success_rate']*100:.1f}%")

    # è®¡ç®—ç»´åº¦å¾—åˆ†
    model_names = runner.get_model_names()
    print(f"\nå¯¹æ¯”çš„æ¨¡å‹: {', '.join(model_names)}")

    dimension_weights = {
        "basic_performance": 1.0,  # å•ç”¨ä¾‹ï¼Œæƒé‡ä¸º1
    }

    categories = list(dimension_weights.keys())
    summaries = metrics_collector.calculate_all_summaries(
        model_names=model_names,
        categories=categories
    )

    # ä¸‰æ¨¡å‹äº¤å‰è¯„ä»·ï¼ˆå¦‚æœå¯ç”¨ï¼‰
    if quality_config.get("enabled", False):
        print("\n" + "=" * 80)
        print("å¼€å§‹ä¸‰æ¨¡å‹äº¤å‰è¯„ä»·...")
        print("=" * 80 + "\n")

        from src.quality.judge_manager import JudgeManager
        from src.quality.minimax_scorer import MiniMaxScoreCalculator

        judge_manager = JudgeManager(config)
        scorer = MiniMaxScoreCalculator()
        quality_stats = metrics_collector.get_quality_statistics()
        print(f"è´¨é‡è¯„ä¼°å®Œæˆ:")
        print(f"  - æ€»è¯„ä¼°æ•°: {quality_stats['overall']['total_evaluations']}")
        print(f"  - æˆåŠŸè¯„ä¼°: {quality_stats['overall']['successful_evaluations']}")
    else:
        scorer = None
        quality_stats = None

    # è®¡ç®—ç»´åº¦å¾—åˆ†
    print("\nè®¡ç®— MiniMax æ ‡å‡†ç»´åº¦å¾—åˆ†...")
    quality_scores = calculate_dimension_scores(model_names, summaries, dimension_weights, metrics_collector)

    # ä¿å­˜è¯„æµ‹æ•°æ®åˆ°JSON
    print("\n" + "=" * 80)
    print("ä¿å­˜è¯„æµ‹æ•°æ®åˆ°JSON...")
    print("=" * 80)
    raw_results = metrics_collector.export_results()
    json_saver = BenchmarkJSONSaver()
    json_path = json_saver.save_evaluation_data(
        statistics=statistics,
        quality_scores=quality_scores,
        summaries=[s.to_dict() for s in summaries],
        raw_results=raw_results,
        dimension_weights=dimension_weights,
        quality_evaluations=metrics_collector.get_quality_statistics() if quality_config.get("enabled", False) else {},
        config=config
    )
    print(f"âœ… JSONæ•°æ®å·²ä¿å­˜: {json_path}")

    # ä»JSONç”ŸæˆMarkdownæŠ¥å‘Š
    print("\n" + "=" * 80)
    print("ä»JSONç”ŸæˆMarkdownæŠ¥å‘Š...")
    print("=" * 80)
    md_generator = MarkdownReportGenerator()
    md_path = md_generator.generate_from_json(json_path)
    print(f"âœ… MarkdownæŠ¥å‘Šå·²ç”Ÿæˆ: {md_path}")

    print("\n" + "=" * 80)
    print("å•ç”¨ä¾‹æµ‹è¯•è¯„æµ‹æ€»ç»“:")
    print("=" * 80)

    for model_name in model_names:
        model_score = quality_scores.get(model_name, {})
        overall_score = model_score.get("overall_score", 0)
        print(f"\n{model_name}:")
        print(f"  ç»¼åˆå¾—åˆ†: {overall_score:.2f}/10")

    # ç”ŸæˆHTMLæŠ¥å‘Šï¼ˆä¿æŒä¸å…¶ä»–æ¨¡å¼ä¸€è‡´ï¼‰
    print("\n" + "=" * 80)
    print("ç”ŸæˆHTMLæŠ¥å‘Š...")
    print("=" * 80 + "\n")

    from src.report.minimax_generator import MiniMaxReportGenerator
    html_report_generator = MiniMaxReportGenerator(config.get("report", {}))

    html_report_path = html_report_generator.generate_minimax_report(
        statistics=statistics,
        quality_scores=quality_scores,
        performance_data=raw_results,
        model_names=model_names,
        dimension_weights=dimension_weights
    )

    print(f"âœ… HTMLæŠ¥å‘Šå·²ç”Ÿæˆ: {html_report_path}")

    print(f"\nâœ… å•ç”¨ä¾‹æµ‹è¯•å®Œæˆï¼")
    print(f"ç»“æŸæ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
    print("=" * 80)


def run_original_test(config):
    """è¿è¡ŒåŸå§‹å¯¹æ¯”æµ‹è¯•ï¼ˆæ¯ç±»åˆ«1ä¸ªç”¨ä¾‹ï¼‰"""
    from src.tests.test_registry import registry
    from src.utils.logger import setup_logger

    print("=" * 80)
    print("DeepSeek vs GLM - æ¯ç±»åˆ«1ç”¨ä¾‹å¯¹æ¯”æµ‹è¯•")
    print(f"å¼€å§‹æ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
    print("=" * 80)
    print()

    # é€‰æ‹©çš„æµ‹è¯•ç”¨ä¾‹ï¼ˆå•ç”¨ä¾‹æµ‹è¯•ï¼‰
    SELECTED_TESTS = {
        "qa_simple": "qa_capital_france"
    }

    print(f"é€‰æ‹©çš„æµ‹è¯•ç±»åˆ«æ•°é‡: {len(SELECTED_TESTS)}")
    for category, test_name in SELECTED_TESTS.items():
        print(f"  - {category}: {test_name}")
    print()

    # åŠ è½½é…ç½®
    if not validate_config(config):
        print("âŒ é…ç½®éªŒè¯å¤±è´¥")
        sys.exit(1)
    print("âœ“ é…ç½®åŠ è½½æˆåŠŸ\n")

    # MiniMax Judge éªŒè¯
    quality_config = config.get("quality", {})
    if quality_config.get("enabled", False):
        validate_minimax_judge(config)

    # è·å–é€‰å®šçš„æµ‹è¯•ç”¨ä¾‹
    test_cases = []
    for category, test_name in SELECTED_TESTS.items():
        # è·å–è¯¥ç±»åˆ«çš„æ‰€æœ‰æµ‹è¯•ç”¨ä¾‹
        category_tests = registry.get_test_cases_by_category(category)
        # æ‰¾åˆ°æŒ‡å®šåç§°çš„æµ‹è¯•ç”¨ä¾‹
        for test in category_tests:
            if test.name == test_name:
                test_cases.append(test)
                break

    print(f"âœ“ åŠ è½½æµ‹è¯•ç”¨ä¾‹: {len(test_cases)} ä¸ª\n")

    # è¿è¡Œæµ‹è¯•
    runner = BenchmarkRunner(config)
    metrics_collector = runner.run_benchmark(
        test_cases=[tc.to_dict() for tc in test_cases],
        show_progress=True
    )

    # ç”ŸæˆæŠ¥å‘Š
    statistics = metrics_collector.get_statistics()
    model_names = runner.get_model_names()
    summaries = metrics_collector.calculate_all_summaries(
        model_names=model_names,
        categories=list(SELECTED_TESTS.keys())
    )
    raw_results = metrics_collector.export_results()
    quality_stats = metrics_collector.get_quality_statistics() if config.get("quality", {}).get("enabled", False) else None

    # ä¿å­˜è¯„æµ‹æ•°æ®åˆ°JSONï¼ˆç»Ÿä¸€æµç¨‹ï¼‰
    print("\n" + "=" * 80)
    print("ä¿å­˜è¯„æµ‹æ•°æ®åˆ°JSON...")
    print("=" * 80)

    # ä¸ºoriginalæ¨¡å¼æ„å»ºç®€åŒ–çš„è´¨é‡è¯„åˆ†ç»“æ„
    quality_scores_simple = {}
    for model_name in model_names:
        model_summaries = [s for s in summaries if s.model_name == model_name]
        if model_summaries:
            # åŸºäºå¹³å‡é€Ÿåº¦è®¡ç®—ç®€åŒ–çš„è´¨é‡è¯„åˆ†
            avg_speed = sum(s.speed_mean for s in model_summaries) / len(model_summaries)
            overall_score = min(10.0, avg_speed / 10.0)

            # æ„å»ºç»´åº¦å¾—åˆ†ï¼ˆä½¿ç”¨æµ‹è¯•ç±»åˆ«ä½œä¸ºç»´åº¦ï¼‰
            dimension_scores = {}
            for summary in model_summaries:
                if summary.test_count > 0:
                    dim_score = min(10.0, summary.speed_mean / 10.0)
                    dimension_scores[summary.category] = dim_score

            quality_scores_simple[model_name] = {
                "overall_score": overall_score,
                "dimension_scores": dimension_scores,
                "grade": "N/A",  # ç®€åŒ–æ¨¡å¼ä¸è®¡ç®—ç­‰çº§
                "rank": 0
            }

    # å¯¼å…¥å¹¶ä½¿ç”¨ JSONSaver
    from src.utils.json_saver import BenchmarkJSONSaver
    json_saver = BenchmarkJSONSaver()

    json_path = json_saver.save_evaluation_data(
        statistics=statistics,
        quality_scores=quality_scores_simple,
        summaries=[s.to_dict() for s in summaries],
        raw_results=raw_results,
        dimension_weights={},  # originalæ¨¡å¼ä¸ä½¿ç”¨ç»´åº¦æƒé‡
        quality_evaluations=quality_stats if quality_stats else {},
        config=config
    )
    print(f"âœ… JSONæ•°æ®å·²ä¿å­˜: {json_path}")

    # ä»JSONç”ŸæˆMarkdownæŠ¥å‘Šï¼ˆç»Ÿä¸€æµç¨‹ï¼‰
    print("\n" + "=" * 80)
    print("ä»JSONç”ŸæˆMarkdownæŠ¥å‘Š...")
    print("=" * 80)

    from src.report.markdown_generator import MarkdownReportGenerator
    md_generator = MarkdownReportGenerator()
    md_path = md_generator.generate_from_json(json_path)
    print(f"âœ… MarkdownæŠ¥å‘Šå·²ç”Ÿæˆ: {md_path}")
    print("\n" + "=" * 80)
    print(f"âœ… å¯¹æ¯”æµ‹è¯•å®Œæˆï¼")
    print(f"ç»“æŸæ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
    print("=" * 80)


# ============ è¾…åŠ©å‡½æ•° ============

def validate_minimax_judge(config):
    """éªŒè¯ MiniMax Judge API è¿æ¥"""
    print("\n" + "=" * 80)
    print("ğŸ” MiniMax Judge æå‰éªŒè¯")
    print("=" * 80 + "\n")

    from src.api.minimax_client import MiniMaxClient

    minimax_api_key = os.environ.get("MINIMAX_API_KEY")
    minimax_base_url = os.environ.get("MINIMAX_BASE_URL", "https://api.minimaxi.com/anthropic")
    minimax_model = os.environ.get("MINIMAX_MODEL", "MiniMax-M2.1")

    if not minimax_api_key:
        print("âŒ MiniMax API Key æœªé…ç½®")
        return False

    print("âœ“ ä»ç¯å¢ƒå˜é‡è¯»å– MiniMax API Key")

    print("åˆå§‹åŒ– MiniMax å®¢æˆ·ç«¯...")
    try:
        # ä½¿ç”¨é¡¹ç›®ä¸­çš„ MiniMaxClientï¼Œå®ƒä½¿ç”¨æ­£ç¡®çš„è®¤è¯æ ¼å¼
        client = MiniMaxClient(
            base_url=minimax_base_url,
            api_key=minimax_api_key,
            model=minimax_model,
            timeout=120
        )

        print("å‘é€é¢„çƒ­è¯·æ±‚...")

        # é¢„çƒ­è¯·æ±‚ 1
        print("  é¢„çƒ­ 1/2ï¼ˆéæµå¼è¯·æ±‚ï¼‰...")
        output1 = client.chat(
            messages=[{"role": "user", "content": "ä½ å¥½"}],
            max_tokens=10
        )
        print(f"    âœ“ å“åº”: {output1[:50]}...")

        # é¢„çƒ­è¯·æ±‚ 2
        print("  é¢„çƒ­ 2/2ï¼ˆå¤šè½®å¯¹è¯ï¼‰...")
        messages2 = [
            {"role": "user", "content": "æˆ‘å«å°æ˜"},
            {"role": "assistant", "content": "ä½ å¥½å°æ˜ï¼"},
            {"role": "user", "content": "è¯·ç”¨ä¸€å¥è¯ä»‹ç»ä½ è‡ªå·±"}
        ]
        output2 = client.chat(
            messages=messages2,
            max_tokens=50
        )
        print(f"    âœ“ å“åº”: {output2[:50]}...")

        print("\nâœ… MiniMax Judge API éªŒè¯æˆåŠŸï¼å¯ä»¥æ­£å¸¸å‚ä¸è´¨é‡è¯„ä¼°ã€‚\n")
        return True

    except Exception as e:
        print(f"\nâŒ MiniMax Judge API éªŒè¯å¤±è´¥: {e}")
        print("\nâš ï¸  MiniMax Judge å°†æ— æ³•å‚ä¸è´¨é‡è¯„ä¼°ï¼Œä½†å…¶ä»– Judge ä»å¯æ­£å¸¸å·¥ä½œã€‚\n")
        return False


def calculate_dimension_scores(model_names, summaries, dimension_weights, metrics_collector):
    """è®¡ç®—ç»´åº¦å¾—åˆ†"""
    quality_scores = {}

    for model_name in model_names:
        model_summaries = [s for s in summaries if s.model_name == model_name]

        dimension_scores = {}
        for dimension in dimension_weights.keys():
            dimension_summaries = [s for s in model_summaries if s.category == dimension]
            if dimension_summaries:
                avg_speed = sum(s.speed_mean for s in dimension_summaries) / len(dimension_summaries)
                score = min(10.0, avg_speed / 10.0)
                dimension_scores[dimension] = score
            else:
                dimension_scores[dimension] = 0.0

        overall_score = sum(
            dimension_scores[d] * dimension_weights[d]
            for d in dimension_weights
        )

        quality_scores[model_name] = {
            "overall_score": overall_score,
            "dimension_scores": dimension_scores
        }

        print(f"\n{model_name}:")
        print(f"  ç»¼åˆå¾—åˆ†: {overall_score:.2f}/10")
        for dimension, score in dimension_scores.items():
            print(f"  - {dimension}: {score:.2f}/10")

    return quality_scores


# ============ ä¸»å‡½æ•° ============

def main():
    """ä¸»å‡½æ•°"""
    parser = argparse.ArgumentParser(
        description="ç»Ÿä¸€è¯„æµ‹è„šæœ¬ - æ”¯æŒ MiniMax æ ‡å‡†è¯„æµ‹å’ŒåŸå§‹å¯¹æ¯”æµ‹è¯•",
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
è¿è¡Œæ¨¡å¼ç¤ºä¾‹ï¼š
  python run_benchmark.py                    # é»˜è®¤ï¼šæ ‡å‡†æ¨¡å¼ï¼ˆ100ä¸ªç”¨ä¾‹ï¼‰
  python run_benchmark.py --mode preview     # é¢„æµ‹è¯•æ¨¡å¼ï¼ˆ4ä¸ªç”¨ä¾‹ï¼‰
  python run_benchmark.py --mode original    # åŸå§‹å¯¹æ¯”æµ‹è¯•æ¨¡å¼ï¼ˆ4ä¸ªç”¨ä¾‹ï¼‰
        """
    )

    parser.add_argument(
        '--mode',
        choices=['standard', 'preview', 'original', 'single'],
        default='standard',
        help='è¿è¡Œæ¨¡å¼: standard(100ä¸ªç”¨ä¾‹), preview(4ä¸ªç”¨ä¾‹), original(åŸå§‹å¯¹æ¯”), single(å•ç”¨ä¾‹)'
    )

    args = parser.parse_args()

    # åŠ è½½é…ç½®
    config = load_config("config.yaml")

    # æ ¹æ®æ¨¡å¼è¿è¡Œä¸åŒçš„æµ‹è¯•
    if args.mode == 'standard':
        run_minimax_standard(config)
    elif args.mode == 'preview':
        run_minimax_preview(config)
    elif args.mode == 'original':
        run_original_test(config)
    elif args.mode == 'single':
        run_single_test(config)


if __name__ == "__main__":
    try:
        main()
    except KeyboardInterrupt:
        print("\n\nâš ï¸  æµ‹è¯•è¢«ç”¨æˆ·ä¸­æ–­")
        sys.exit(130)
    except Exception as e:
        print(f"\nâŒ å‘ç”Ÿé”™è¯¯: {e}")
        import traceback
        traceback.print_exc()
        sys.exit(1)
